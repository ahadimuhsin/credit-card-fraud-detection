{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\n",
    "from imblearn.metrics import geometric_mean_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
      "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
      "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',\n",
      "       'Class'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('F:\\Kuliah\\Semester 7\\Data Mining\\Data Dow Jones\\creditcard.csv', sep=',')\n",
    "#data diambil dari kaggle.com/mlg-ulb/creditcardfraud\n",
    "\n",
    "data2 = data\n",
    "\n",
    "print (data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Time         V1         V2        V3        V4        V5  \\\n",
      "0            0.0  -1.359807  -0.072781  2.536347  1.378155 -0.338321   \n",
      "1            0.0   1.191857   0.266151  0.166480  0.448154  0.060018   \n",
      "2            1.0  -1.358354  -1.340163  1.773209  0.379780 -0.503198   \n",
      "3            1.0  -0.966272  -0.185226  1.792993 -0.863291 -0.010309   \n",
      "4            2.0  -1.158233   0.877737  1.548718  0.403034 -0.407193   \n",
      "5            2.0  -0.425966   0.960523  1.141109 -0.168252  0.420987   \n",
      "6            4.0   1.229658   0.141004  0.045371  1.202613  0.191881   \n",
      "7            7.0  -0.644269   1.417964  1.074380 -0.492199  0.948934   \n",
      "8            7.0  -0.894286   0.286157 -0.113192 -0.271526  2.669599   \n",
      "9            9.0  -0.338262   1.119593  1.044367 -0.222187  0.499361   \n",
      "10          10.0   1.449044  -1.176339  0.913860 -1.375667 -1.971383   \n",
      "11          10.0   0.384978   0.616109 -0.874300 -0.094019  2.924584   \n",
      "12          10.0   1.249999  -1.221637  0.383930 -1.234899 -1.485419   \n",
      "13          11.0   1.069374   0.287722  0.828613  2.712520 -0.178398   \n",
      "14          12.0  -2.791855  -0.327771  1.641750  1.767473 -0.136588   \n",
      "15          12.0  -0.752417   0.345485  2.057323 -1.468643 -1.158394   \n",
      "16          12.0   1.103215  -0.040296  1.267332  1.289091 -0.735997   \n",
      "17          13.0  -0.436905   0.918966  0.924591 -0.727219  0.915679   \n",
      "18          14.0  -5.401258  -5.450148  1.186305  1.736239  3.049106   \n",
      "19          15.0   1.492936  -1.029346  0.454795 -1.438026 -1.555434   \n",
      "20          16.0   0.694885  -1.361819  1.029221  0.834159 -1.191209   \n",
      "21          17.0   0.962496   0.328461 -0.171479  2.109204  1.129566   \n",
      "22          18.0   1.166616   0.502120 -0.067300  2.261569  0.428804   \n",
      "23          18.0   0.247491   0.277666  1.185471 -0.092603 -1.314394   \n",
      "24          22.0  -1.946525  -0.044901 -0.405570 -1.013057  2.941968   \n",
      "25          22.0  -2.074295  -0.121482  1.322021  0.410008  0.295198   \n",
      "26          23.0   1.173285   0.353498  0.283905  1.133563 -0.172577   \n",
      "27          23.0   1.322707  -0.174041  0.434555  0.576038 -0.836758   \n",
      "28          23.0  -0.414289   0.905437  1.727453  1.473471  0.007443   \n",
      "29          23.0   1.059387  -0.175319  1.266130  1.186110 -0.786002   \n",
      "...          ...        ...        ...       ...       ...       ...   \n",
      "284777  172764.0   2.079137  -0.028723 -1.343392  0.358000 -0.045791   \n",
      "284778  172764.0  -0.764523   0.588379 -0.907599 -0.418847  0.901528   \n",
      "284779  172766.0   1.975178  -0.616244 -2.628295 -0.406246  2.327804   \n",
      "284780  172766.0  -1.727503   1.108356  2.219561  1.148583 -0.884199   \n",
      "284781  172766.0  -1.139015  -0.155510  1.894478 -1.138957  1.451777   \n",
      "284782  172767.0  -0.268061   2.540315 -1.400915  4.846661  0.639105   \n",
      "284783  172768.0  -1.796092   1.929178 -2.828417 -1.689844  2.199572   \n",
      "284784  172768.0  -0.669662   0.923769 -1.543167 -1.560729  2.833960   \n",
      "284785  172768.0   0.032887   0.545338 -1.185844 -1.729828  2.932315   \n",
      "284786  172768.0  -2.076175   2.142238 -2.522704 -1.888063  1.982785   \n",
      "284787  172769.0  -1.029719  -1.110670 -0.636179 -0.840816  2.424360   \n",
      "284788  172770.0   2.007418  -0.280235 -0.208113  0.335261 -0.715798   \n",
      "284789  172770.0  -0.446951   1.302212 -0.168583  0.981577  0.578957   \n",
      "284790  172771.0  -0.515513   0.971950 -1.014580 -0.677037  0.912430   \n",
      "284791  172774.0  -0.863506   0.874701  0.420358 -0.530365  0.356561   \n",
      "284792  172774.0  -0.724123   1.485216 -1.132218 -0.607190  0.709499   \n",
      "284793  172775.0   1.971002  -0.699067 -1.697541 -0.617643  1.718797   \n",
      "284794  172777.0  -1.266580  -0.400461  0.956221 -0.723919  1.531993   \n",
      "284795  172778.0 -12.516732  10.187818 -8.476671 -2.510473 -4.586669   \n",
      "284796  172780.0   1.884849  -0.143540 -0.999943  1.506772 -0.035300   \n",
      "284797  172782.0  -0.241923   0.712247  0.399806 -0.463406  0.244531   \n",
      "284798  172782.0   0.219529   0.881246 -0.635891  0.960928 -0.152971   \n",
      "284799  172783.0  -1.775135  -0.004235  1.189786  0.331096  1.196063   \n",
      "284800  172784.0   2.039560  -0.175233 -1.196825  0.234580 -0.008713   \n",
      "284801  172785.0   0.120316   0.931005 -0.546012 -0.745097  1.130314   \n",
      "284802  172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473   \n",
      "284803  172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229   \n",
      "284804  172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515   \n",
      "284805  172788.0  -0.240440   0.530483  0.702510  0.689799 -0.377961   \n",
      "284806  172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546   \n",
      "\n",
      "              V6        V7        V8        V9  ...         V21       V22  \\\n",
      "0       0.462388  0.239599  0.098698  0.363787  ...   -0.018307  0.277838   \n",
      "1      -0.082361 -0.078803  0.085102 -0.255425  ...   -0.225775 -0.638672   \n",
      "2       1.800499  0.791461  0.247676 -1.514654  ...    0.247998  0.771679   \n",
      "3       1.247203  0.237609  0.377436 -1.387024  ...   -0.108300  0.005274   \n",
      "4       0.095921  0.592941 -0.270533  0.817739  ...   -0.009431  0.798278   \n",
      "5      -0.029728  0.476201  0.260314 -0.568671  ...   -0.208254 -0.559825   \n",
      "6       0.272708 -0.005159  0.081213  0.464960  ...   -0.167716 -0.270710   \n",
      "7       0.428118  1.120631 -3.807864  0.615375  ...    1.943465 -1.015455   \n",
      "8       3.721818  0.370145  0.851084 -0.392048  ...   -0.073425 -0.268092   \n",
      "9      -0.246761  0.651583  0.069539 -0.736727  ...   -0.246914 -0.633753   \n",
      "10     -0.629152 -1.423236  0.048456 -1.720408  ...   -0.009302  0.313894   \n",
      "11      3.317027  0.470455  0.538247 -0.558895  ...    0.049924  0.238422   \n",
      "12     -0.753230 -0.689405 -0.227487 -2.094011  ...   -0.231809 -0.483285   \n",
      "13      0.337544 -0.096717  0.115982 -0.221083  ...   -0.036876  0.074412   \n",
      "14      0.807596 -0.422911 -1.907107  0.755713  ...    1.151663  0.222182   \n",
      "15     -0.077850 -0.608581  0.003603 -0.436167  ...    0.499625  1.353650   \n",
      "16      0.288069 -0.586057  0.189380  0.782333  ...   -0.024612  0.196002   \n",
      "17     -0.127867  0.707642  0.087962 -0.665271  ...   -0.194796 -0.672638   \n",
      "18     -1.763406 -1.559738  0.160842  1.233090  ...   -0.503600  0.984460   \n",
      "19     -0.720961 -1.080664 -0.053127 -1.978682  ...   -0.177650 -0.175074   \n",
      "20      1.309109 -0.878586  0.445290 -0.446196  ...   -0.295583 -0.571955   \n",
      "21      1.696038  0.107712  0.521502 -1.191311  ...    0.143997  0.402492   \n",
      "22      0.089474  0.241147  0.138082 -0.989162  ...    0.018702 -0.061972   \n",
      "23     -0.150116 -0.946365 -1.617935  1.544071  ...    1.650180  0.200454   \n",
      "24      2.955053 -0.063063  0.855546  0.049967  ...   -0.579526 -0.799229   \n",
      "25     -0.959537  0.543985 -0.104627  0.475664  ...   -0.403639 -0.227404   \n",
      "26     -0.916054  0.369025 -0.327260 -0.246651  ...    0.067003  0.227812   \n",
      "27     -0.831083 -0.264905 -0.220982 -1.071425  ...   -0.284376 -0.323357   \n",
      "28     -0.200331  0.740228 -0.029247 -0.593392  ...    0.077237  0.457331   \n",
      "29      0.578435 -0.767084  0.401046  0.699500  ...    0.013676  0.213734   \n",
      "...          ...       ...       ...       ...  ...         ...       ...   \n",
      "284777 -1.345452  0.227476 -0.378355  0.665911  ...    0.235758  0.829758   \n",
      "284778 -0.760802  0.758545  0.414698 -0.730854  ...    0.003530 -0.431876   \n",
      "284779  3.664740 -0.533297  0.842937  1.128798  ...    0.086043  0.543613   \n",
      "284780  0.793083 -0.527298  0.866429  0.853819  ...   -0.094708  0.236818   \n",
      "284781  0.093598  0.191353  0.092211 -0.062621  ...   -0.191027 -0.631658   \n",
      "284782  0.186479 -0.045911  0.936448 -2.419986  ...   -0.263889 -0.857904   \n",
      "284783  3.123732 -0.270714  1.657495  0.465804  ...    0.271170  1.145750   \n",
      "284784  3.240843  0.181576  1.282746 -0.893890  ...    0.183856  0.202670   \n",
      "284785  3.401529  0.337434  0.925377 -0.165663  ...   -0.266113 -0.716336   \n",
      "284786  3.732950 -1.217430 -0.536644  0.272867  ...    2.016666 -1.588269   \n",
      "284787 -2.956733  0.283610 -0.332656 -0.247488  ...    0.353722  0.488487   \n",
      "284788 -0.751373 -0.458972 -0.140140  0.959971  ...   -0.208260 -0.430347   \n",
      "284789 -0.605641  1.253430 -1.042610 -0.417116  ...    0.851800  0.305268   \n",
      "284790 -0.316187  0.396137  0.532364 -0.224606  ...   -0.280302 -0.849919   \n",
      "284791 -1.046238  0.757051  0.230473 -0.506856  ...   -0.108846 -0.480820   \n",
      "284792 -0.482638  0.548393  0.343003 -0.226323  ...    0.414621  1.307511   \n",
      "284793  3.911336 -1.259306  1.056209  1.315006  ...    0.188758  0.694418   \n",
      "284794 -1.788600  0.314741  0.004704  0.013857  ...   -0.157831 -0.883365   \n",
      "284795 -1.394465 -3.632516  5.498583  4.893089  ...   -0.944759 -1.565026   \n",
      "284796 -0.613638  0.190241 -0.249058  0.666458  ...    0.144008  0.634646   \n",
      "284797 -1.343668  0.929369 -0.206210  0.106234  ...   -0.228876 -0.514376   \n",
      "284798 -1.014307  0.427126  0.121340 -0.285670  ...    0.099936  0.337120   \n",
      "284799  5.519980 -1.518185  2.080825  1.159498  ...    0.103302  0.654850   \n",
      "284800 -0.726571  0.017050 -0.118228  0.435402  ...   -0.268048 -0.717211   \n",
      "284801 -0.235973  0.812722  0.115093 -0.204064  ...   -0.314205 -0.808520   \n",
      "284802 -2.606837 -4.918215  7.305334  1.914428  ...    0.213454  0.111864   \n",
      "284803  1.058415  0.024330  0.294869  0.584800  ...    0.214205  0.924384   \n",
      "284804  3.031260 -0.296827  0.708417  0.432454  ...    0.232045  0.578229   \n",
      "284805  0.623708 -0.686180  0.679145  0.392087  ...    0.265245  0.800049   \n",
      "284806 -0.649617  1.577006 -0.414650  0.486180  ...    0.261057  0.643078   \n",
      "\n",
      "             V23       V24       V25       V26       V27       V28  Amount  \\\n",
      "0      -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62   \n",
      "1       0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69   \n",
      "2       0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66   \n",
      "3      -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50   \n",
      "4      -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   69.99   \n",
      "5      -0.026398 -0.371427 -0.232794  0.105915  0.253844  0.081080    3.67   \n",
      "6      -0.154104 -0.780055  0.750137 -0.257237  0.034507  0.005168    4.99   \n",
      "7       0.057504 -0.649709 -0.415267 -0.051634 -1.206921 -1.085339   40.80   \n",
      "8      -0.204233  1.011592  0.373205 -0.384157  0.011747  0.142404   93.20   \n",
      "9      -0.120794 -0.385050 -0.069733  0.094199  0.246219  0.083076    3.68   \n",
      "10      0.027740  0.500512  0.251367 -0.129478  0.042850  0.016253    7.80   \n",
      "11      0.009130  0.996710 -0.767315 -0.492208  0.042472 -0.054337    9.99   \n",
      "12      0.084668  0.392831  0.161135 -0.354990  0.026416  0.042422  121.50   \n",
      "13     -0.071407  0.104744  0.548265  0.104094  0.021491  0.021293   27.50   \n",
      "14      1.020586  0.028317 -0.232746 -0.235557 -0.164778 -0.030154   58.80   \n",
      "15     -0.256573 -0.065084 -0.039124 -0.087086 -0.180998  0.129394   15.99   \n",
      "16      0.013802  0.103758  0.364298 -0.382261  0.092809  0.037051   12.99   \n",
      "17     -0.156858 -0.888386 -0.342413 -0.049027  0.079692  0.131024    0.89   \n",
      "18      2.458589  0.042119 -0.481631 -0.621272  0.392053  0.949594   46.80   \n",
      "19      0.040002  0.295814  0.332931 -0.220385  0.022298  0.007602    5.00   \n",
      "20     -0.050881 -0.304215  0.072001 -0.422234  0.086553  0.063499  231.71   \n",
      "21     -0.048508 -1.371866  0.390814  0.199964  0.016371 -0.014605   34.09   \n",
      "22     -0.103855 -0.370415  0.603200  0.108556 -0.040521 -0.011418    2.28   \n",
      "23     -0.185353  0.423073  0.820591 -0.227632  0.336634  0.250475   22.75   \n",
      "24      0.870300  0.983421  0.321201  0.149650  0.707519  0.014600    0.89   \n",
      "25      0.742435  0.398535  0.249212  0.274404  0.359969  0.243232   26.43   \n",
      "26     -0.150487  0.435045  0.724825 -0.337082  0.016368  0.030041   41.88   \n",
      "27     -0.037710  0.347151  0.559639 -0.280158  0.042335  0.028822   16.00   \n",
      "28     -0.038500  0.642522 -0.183891 -0.277464  0.182687  0.152665   33.00   \n",
      "29      0.014462  0.002951  0.294638 -0.395070  0.081461  0.024220   12.99   \n",
      "...          ...       ...       ...       ...       ...       ...     ...   \n",
      "284777 -0.002063  0.001344  0.262183 -0.105327 -0.022363 -0.060283    1.00   \n",
      "284778  0.141759  0.587119 -0.200998  0.267337 -0.152951 -0.065285   80.00   \n",
      "284779 -0.032129  0.768379  0.477688 -0.031833  0.014151 -0.066542   25.00   \n",
      "284780 -0.204280  1.158185  0.627801 -0.399981  0.510818  0.233265   30.00   \n",
      "284781 -0.147249  0.212931  0.354257 -0.241068 -0.161717 -0.149188   13.00   \n",
      "284782  0.235172 -0.681794 -0.668894  0.044657 -0.066751 -0.072447   12.82   \n",
      "284783  0.084783  0.721269 -0.529906 -0.240117  0.129126 -0.080620   11.46   \n",
      "284784 -0.373023  0.651122  1.073823  0.844590 -0.286676 -0.187719   40.00   \n",
      "284785  0.108519  0.688519 -0.460220  0.161939  0.265368  0.090245    1.79   \n",
      "284786  0.588482  0.632444 -0.201064  0.199251  0.438657  0.172923    8.95   \n",
      "284787  0.293632  0.107812 -0.935586  1.138216  0.025271  0.255347    9.99   \n",
      "284788  0.416765  0.064819 -0.608337  0.268436 -0.028069 -0.041367    3.99   \n",
      "284789 -0.148093 -0.038712  0.010209 -0.362666  0.503092  0.229921   60.50   \n",
      "284790  0.300245  0.000607 -0.376379  0.128660 -0.015205 -0.021486    9.81   \n",
      "284791 -0.074513 -0.003988 -0.113149  0.280378 -0.077310  0.023079   20.32   \n",
      "284792 -0.059545  0.242669 -0.665424 -0.269869 -0.170579 -0.030692    3.99   \n",
      "284793  0.163002  0.726365 -0.058282 -0.191813  0.061858 -0.043716    4.99   \n",
      "284794  0.088485 -0.076790 -0.095833  0.132720 -0.028468  0.126494    0.89   \n",
      "284795  0.890675 -1.253276  1.786717  0.320763  2.090712  1.232864    9.87   \n",
      "284796 -0.042114 -0.053206  0.316403 -0.461441  0.018265 -0.041068   60.00   \n",
      "284797  0.279598  0.371441 -0.559238  0.113144  0.131507  0.081265    5.49   \n",
      "284798  0.251791  0.057688 -1.508368  0.144023  0.181205  0.215243   24.05   \n",
      "284799 -0.348929  0.745323  0.704545 -0.127579  0.454379  0.130308   79.99   \n",
      "284800  0.297930 -0.359769 -0.315610  0.201114 -0.080826 -0.075071    2.68   \n",
      "284801  0.050343  0.102800 -0.435870  0.124079  0.217940  0.068803    2.69   \n",
      "284802  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731    0.77   \n",
      "284803  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   24.79   \n",
      "284804 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   67.88   \n",
      "284805 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   10.00   \n",
      "284806  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649  217.00   \n",
      "\n",
      "        Class  \n",
      "0           0  \n",
      "1           0  \n",
      "2           0  \n",
      "3           0  \n",
      "4           0  \n",
      "5           0  \n",
      "6           0  \n",
      "7           0  \n",
      "8           0  \n",
      "9           0  \n",
      "10          0  \n",
      "11          0  \n",
      "12          0  \n",
      "13          0  \n",
      "14          0  \n",
      "15          0  \n",
      "16          0  \n",
      "17          0  \n",
      "18          0  \n",
      "19          0  \n",
      "20          0  \n",
      "21          0  \n",
      "22          0  \n",
      "23          0  \n",
      "24          0  \n",
      "25          0  \n",
      "26          0  \n",
      "27          0  \n",
      "28          0  \n",
      "29          0  \n",
      "...       ...  \n",
      "284777      0  \n",
      "284778      0  \n",
      "284779      0  \n",
      "284780      0  \n",
      "284781      0  \n",
      "284782      0  \n",
      "284783      0  \n",
      "284784      0  \n",
      "284785      0  \n",
      "284786      0  \n",
      "284787      0  \n",
      "284788      0  \n",
      "284789      0  \n",
      "284790      0  \n",
      "284791      0  \n",
      "284792      0  \n",
      "284793      0  \n",
      "284794      0  \n",
      "284795      0  \n",
      "284796      0  \n",
      "284797      0  \n",
      "284798      0  \n",
      "284799      0  \n",
      "284800      0  \n",
      "284801      0  \n",
      "284802      0  \n",
      "284803      0  \n",
      "284804      0  \n",
      "284805      0  \n",
      "284806      0  \n",
      "\n",
      "[284807 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "print (data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284807, 31)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>284807.000000</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>94813.859575</td>\n",
       "      <td>1.165980e-15</td>\n",
       "      <td>3.416908e-16</td>\n",
       "      <td>-1.373150e-15</td>\n",
       "      <td>2.086869e-15</td>\n",
       "      <td>9.604066e-16</td>\n",
       "      <td>1.490107e-15</td>\n",
       "      <td>-5.556467e-16</td>\n",
       "      <td>1.177556e-16</td>\n",
       "      <td>-2.406455e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>1.656562e-16</td>\n",
       "      <td>-3.444850e-16</td>\n",
       "      <td>2.578648e-16</td>\n",
       "      <td>4.471968e-15</td>\n",
       "      <td>5.340915e-16</td>\n",
       "      <td>1.687098e-15</td>\n",
       "      <td>-3.666453e-16</td>\n",
       "      <td>-1.220404e-16</td>\n",
       "      <td>88.349619</td>\n",
       "      <td>0.001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>47488.145955</td>\n",
       "      <td>1.958696e+00</td>\n",
       "      <td>1.651309e+00</td>\n",
       "      <td>1.516255e+00</td>\n",
       "      <td>1.415869e+00</td>\n",
       "      <td>1.380247e+00</td>\n",
       "      <td>1.332271e+00</td>\n",
       "      <td>1.237094e+00</td>\n",
       "      <td>1.194353e+00</td>\n",
       "      <td>1.098632e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>7.345240e-01</td>\n",
       "      <td>7.257016e-01</td>\n",
       "      <td>6.244603e-01</td>\n",
       "      <td>6.056471e-01</td>\n",
       "      <td>5.212781e-01</td>\n",
       "      <td>4.822270e-01</td>\n",
       "      <td>4.036325e-01</td>\n",
       "      <td>3.300833e-01</td>\n",
       "      <td>250.120109</td>\n",
       "      <td>0.041527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.640751e+01</td>\n",
       "      <td>-7.271573e+01</td>\n",
       "      <td>-4.832559e+01</td>\n",
       "      <td>-5.683171e+00</td>\n",
       "      <td>-1.137433e+02</td>\n",
       "      <td>-2.616051e+01</td>\n",
       "      <td>-4.355724e+01</td>\n",
       "      <td>-7.321672e+01</td>\n",
       "      <td>-1.343407e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.483038e+01</td>\n",
       "      <td>-1.093314e+01</td>\n",
       "      <td>-4.480774e+01</td>\n",
       "      <td>-2.836627e+00</td>\n",
       "      <td>-1.029540e+01</td>\n",
       "      <td>-2.604551e+00</td>\n",
       "      <td>-2.256568e+01</td>\n",
       "      <td>-1.543008e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>54201.500000</td>\n",
       "      <td>-9.203734e-01</td>\n",
       "      <td>-5.985499e-01</td>\n",
       "      <td>-8.903648e-01</td>\n",
       "      <td>-8.486401e-01</td>\n",
       "      <td>-6.915971e-01</td>\n",
       "      <td>-7.682956e-01</td>\n",
       "      <td>-5.540759e-01</td>\n",
       "      <td>-2.086297e-01</td>\n",
       "      <td>-6.430976e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.283949e-01</td>\n",
       "      <td>-5.423504e-01</td>\n",
       "      <td>-1.618463e-01</td>\n",
       "      <td>-3.545861e-01</td>\n",
       "      <td>-3.171451e-01</td>\n",
       "      <td>-3.269839e-01</td>\n",
       "      <td>-7.083953e-02</td>\n",
       "      <td>-5.295979e-02</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>84692.000000</td>\n",
       "      <td>1.810880e-02</td>\n",
       "      <td>6.548556e-02</td>\n",
       "      <td>1.798463e-01</td>\n",
       "      <td>-1.984653e-02</td>\n",
       "      <td>-5.433583e-02</td>\n",
       "      <td>-2.741871e-01</td>\n",
       "      <td>4.010308e-02</td>\n",
       "      <td>2.235804e-02</td>\n",
       "      <td>-5.142873e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.945017e-02</td>\n",
       "      <td>6.781943e-03</td>\n",
       "      <td>-1.119293e-02</td>\n",
       "      <td>4.097606e-02</td>\n",
       "      <td>1.659350e-02</td>\n",
       "      <td>-5.213911e-02</td>\n",
       "      <td>1.342146e-03</td>\n",
       "      <td>1.124383e-02</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>139320.500000</td>\n",
       "      <td>1.315642e+00</td>\n",
       "      <td>8.037239e-01</td>\n",
       "      <td>1.027196e+00</td>\n",
       "      <td>7.433413e-01</td>\n",
       "      <td>6.119264e-01</td>\n",
       "      <td>3.985649e-01</td>\n",
       "      <td>5.704361e-01</td>\n",
       "      <td>3.273459e-01</td>\n",
       "      <td>5.971390e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.863772e-01</td>\n",
       "      <td>5.285536e-01</td>\n",
       "      <td>1.476421e-01</td>\n",
       "      <td>4.395266e-01</td>\n",
       "      <td>3.507156e-01</td>\n",
       "      <td>2.409522e-01</td>\n",
       "      <td>9.104512e-02</td>\n",
       "      <td>7.827995e-02</td>\n",
       "      <td>77.165000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>172792.000000</td>\n",
       "      <td>2.454930e+00</td>\n",
       "      <td>2.205773e+01</td>\n",
       "      <td>9.382558e+00</td>\n",
       "      <td>1.687534e+01</td>\n",
       "      <td>3.480167e+01</td>\n",
       "      <td>7.330163e+01</td>\n",
       "      <td>1.205895e+02</td>\n",
       "      <td>2.000721e+01</td>\n",
       "      <td>1.559499e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>2.720284e+01</td>\n",
       "      <td>1.050309e+01</td>\n",
       "      <td>2.252841e+01</td>\n",
       "      <td>4.584549e+00</td>\n",
       "      <td>7.519589e+00</td>\n",
       "      <td>3.517346e+00</td>\n",
       "      <td>3.161220e+01</td>\n",
       "      <td>3.384781e+01</td>\n",
       "      <td>25691.160000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Time            V1            V2            V3            V4  \\\n",
       "count  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean    94813.859575  1.165980e-15  3.416908e-16 -1.373150e-15  2.086869e-15   \n",
       "std     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00   \n",
       "min         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00   \n",
       "25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01   \n",
       "50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02   \n",
       "75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01   \n",
       "max    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01   \n",
       "\n",
       "                 V5            V6            V7            V8            V9  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   9.604066e-16  1.490107e-15 -5.556467e-16  1.177556e-16 -2.406455e-15   \n",
       "std    1.380247e+00  1.332271e+00  1.237094e+00  1.194353e+00  1.098632e+00   \n",
       "min   -1.137433e+02 -2.616051e+01 -4.355724e+01 -7.321672e+01 -1.343407e+01   \n",
       "25%   -6.915971e-01 -7.682956e-01 -5.540759e-01 -2.086297e-01 -6.430976e-01   \n",
       "50%   -5.433583e-02 -2.741871e-01  4.010308e-02  2.235804e-02 -5.142873e-02   \n",
       "75%    6.119264e-01  3.985649e-01  5.704361e-01  3.273459e-01  5.971390e-01   \n",
       "max    3.480167e+01  7.330163e+01  1.205895e+02  2.000721e+01  1.559499e+01   \n",
       "\n",
       "           ...                 V21           V22           V23           V24  \\\n",
       "count      ...        2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean       ...        1.656562e-16 -3.444850e-16  2.578648e-16  4.471968e-15   \n",
       "std        ...        7.345240e-01  7.257016e-01  6.244603e-01  6.056471e-01   \n",
       "min        ...       -3.483038e+01 -1.093314e+01 -4.480774e+01 -2.836627e+00   \n",
       "25%        ...       -2.283949e-01 -5.423504e-01 -1.618463e-01 -3.545861e-01   \n",
       "50%        ...       -2.945017e-02  6.781943e-03 -1.119293e-02  4.097606e-02   \n",
       "75%        ...        1.863772e-01  5.285536e-01  1.476421e-01  4.395266e-01   \n",
       "max        ...        2.720284e+01  1.050309e+01  2.252841e+01  4.584549e+00   \n",
       "\n",
       "                V25           V26           V27           V28         Amount  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  284807.000000   \n",
       "mean   5.340915e-16  1.687098e-15 -3.666453e-16 -1.220404e-16      88.349619   \n",
       "std    5.212781e-01  4.822270e-01  4.036325e-01  3.300833e-01     250.120109   \n",
       "min   -1.029540e+01 -2.604551e+00 -2.256568e+01 -1.543008e+01       0.000000   \n",
       "25%   -3.171451e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02       5.600000   \n",
       "50%    1.659350e-02 -5.213911e-02  1.342146e-03  1.124383e-02      22.000000   \n",
       "75%    3.507156e-01  2.409522e-01  9.104512e-02  7.827995e-02      77.165000   \n",
       "max    7.519589e+00  3.517346e+00  3.161220e+01  3.384781e+01   25691.160000   \n",
       "\n",
       "               Class  \n",
       "count  284807.000000  \n",
       "mean        0.001727  \n",
       "std         0.041527  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Frauds 99.83 % of the dataset\n",
      "Frauds 0.17 % of the dataset\n"
     ]
    }
   ],
   "source": [
    "##memeriksa persentase data transaksi yang termasuk non fraud dan fraud pada dataset\n",
    "print('No Frauds', round(data['Class'].value_counts()[0]/len(data) * 100,2), '% of the dataset')\n",
    "print('Frauds', round(data['Class'].value_counts()[1]/len(data) * 100,2), '% of the dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class\n",
       "0    284315\n",
       "1       492\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('Class').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##melakukan feature scaling\n",
    "##Feature Scaling adalah suatu cara untuk membuat numerical data \n",
    "##pada dataset memiliki rentang nilai (scale) yang sama. \n",
    "##Tidak ada lagi satu variabel data yang mendominasi variabel data lainnya.\n",
    "\n",
    "##membuat sub-sample dengan data 50/50\n",
    "##men-scale kolom Time dan Amount, karena kolom yang lain sudah di-scale\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "# RobustScaler menghilangkan median dan men-scale data berdasarkan quartile range-nya\n",
    "# StandardScaler menghilangkan mean dan men-scale data ke unit variance\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "rob_scaler = RobustScaler()\n",
    "\n",
    "data['scaled_amount'] = rob_scaler.fit_transform(data['Amount'].values.reshape(-1,1))\n",
    "#data['scaled_time'] = rob_scaler.fit_transform(data['Time'].values.reshape(-1,1))\n",
    "\n",
    "data.drop(['Time','Amount'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scaled_amount</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.783274</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.269825</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.983721</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.418291</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.670579</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   scaled_amount        V1        V2        V3        V4        V5        V6  \\\n",
       "0       1.783274 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388   \n",
       "1      -0.269825  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361   \n",
       "2       4.983721 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499   \n",
       "3       1.418291 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203   \n",
       "4       0.670579 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921   \n",
       "\n",
       "         V7        V8        V9  ...         V20       V21       V22  \\\n",
       "0  0.239599  0.098698  0.363787  ...    0.251412 -0.018307  0.277838   \n",
       "1 -0.078803  0.085102 -0.255425  ...   -0.069083 -0.225775 -0.638672   \n",
       "2  0.791461  0.247676 -1.514654  ...    0.524980  0.247998  0.771679   \n",
       "3  0.237609  0.377436 -1.387024  ...   -0.208038 -0.108300  0.005274   \n",
       "4  0.592941 -0.270533  0.817739  ...    0.408542 -0.009431  0.798278   \n",
       "\n",
       "        V23       V24       V25       V26       V27       V28  Class  \n",
       "0 -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053      0  \n",
       "1  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724      0  \n",
       "2  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752      0  \n",
       "3 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458      0  \n",
       "4 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153      0  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mengubah nama kolom Time Amount menjadi scaled_time dan scaled_amount\n",
    "scaled_amount = data['scaled_amount']\n",
    "#scaled_time = data['scaled_time']\n",
    "\n",
    "data.drop(['scaled_amount'], axis=1, inplace=True)\n",
    "data.insert(0, 'scaled_amount', scaled_amount)\n",
    "#data.insert(1, 'scaled_time', scaled_time)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Frauds 99.83 % of the dataset\n",
      "Frauds 0.17 % of the dataset\n",
      "Train: [ 87247  90855  40085 ... 236247 199115 212516] Test: [172818  67471  67694 ...  29249  12061 260434]\n",
      "Train: [93486  5606 21697 ... 10690 46918 57615] Test: [149951 185116 164361 ... 127957 133436 243340]\n",
      "Train: [ 43768 204636 124578 ... 166081  31506 273542] Test: [ 76248 156322  15978 ...  87030 203171  20439]\n",
      "Train: [104338 223911  42473 ... 139759  84543  71334] Test: [198111 275471 214589 ...  52614 125156  94648]\n",
      "Train: [172229 196051 154684 ...  85496  82871  30442] Test: [218704 159743 160019 ...  39048 129315  55212]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Label Distributions: \n",
      "\n",
      "[0.76004872 0.23995128]\n",
      "[9.99928726e-01 7.12740594e-05]\n"
     ]
    }
   ],
   "source": [
    "##Splitting Data dari Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "print('No Frauds', round(data['Class'].value_counts()[0]/len(data) * 100,2), '% of the dataset')\n",
    "print('Frauds', round(data['Class'].value_counts()[1]/len(data) * 100,2), '% of the dataset')\n",
    "\n",
    "X = data.drop('Class', axis=1)\n",
    "y = data['Class']\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    print(\"Train:\", train_index, \"Test:\", test_index)\n",
    "    original_Xtrain, original_Xtest = X.iloc[train_index], X.iloc[test_index]\n",
    "    original_ytrain, original_ytest = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "# We already have X_train and y_train for undersample data thats why I am using original to distinguish and to not overwrite these variables.\n",
    "# original_Xtrain, original_Xtest, original_ytrain, original_ytest = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the Distribution of the labels\n",
    "\n",
    "\n",
    "# Turn into an array\n",
    "original_Xtrain = original_Xtrain.values\n",
    "original_Xtest = original_Xtest.values\n",
    "original_ytrain = original_ytrain.values\n",
    "original_ytest = original_ytest.values\n",
    "\n",
    "original_Xtrain.shape\n",
    "\n",
    "# See if both the train and test label distribution are similarly distributed\n",
    "train_unique_label, train_counts_label = np.unique(original_ytrain, return_counts=True)\n",
    "test_unique_label, test_counts_label = np.unique(original_ytest, return_counts=True)\n",
    "print('-' * 100)\n",
    "\n",
    "print('Label Distributions: \\n')\n",
    "print(train_counts_label/ len(original_ytrain))\n",
    "print(test_counts_label/ len(original_ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scaled_amount</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>210893</th>\n",
       "      <td>-0.273178</td>\n",
       "      <td>2.078547</td>\n",
       "      <td>-0.132505</td>\n",
       "      <td>-2.092574</td>\n",
       "      <td>0.118425</td>\n",
       "      <td>0.513681</td>\n",
       "      <td>-0.852285</td>\n",
       "      <td>0.399962</td>\n",
       "      <td>-0.188309</td>\n",
       "      <td>0.196690</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.328672</td>\n",
       "      <td>0.125959</td>\n",
       "      <td>0.410059</td>\n",
       "      <td>0.030725</td>\n",
       "      <td>0.741450</td>\n",
       "      <td>0.257435</td>\n",
       "      <td>0.783731</td>\n",
       "      <td>-0.140019</td>\n",
       "      <td>-0.094269</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79536</th>\n",
       "      <td>-0.293440</td>\n",
       "      <td>-0.264869</td>\n",
       "      <td>3.386140</td>\n",
       "      <td>-3.454997</td>\n",
       "      <td>4.367629</td>\n",
       "      <td>3.336060</td>\n",
       "      <td>-2.053918</td>\n",
       "      <td>0.256890</td>\n",
       "      <td>-2.957235</td>\n",
       "      <td>-2.855797</td>\n",
       "      <td>...</td>\n",
       "      <td>0.482513</td>\n",
       "      <td>-1.394504</td>\n",
       "      <td>-0.166029</td>\n",
       "      <td>-1.452081</td>\n",
       "      <td>-0.251815</td>\n",
       "      <td>1.243461</td>\n",
       "      <td>0.452787</td>\n",
       "      <td>0.132218</td>\n",
       "      <td>0.424599</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143250</th>\n",
       "      <td>-0.028645</td>\n",
       "      <td>-1.731864</td>\n",
       "      <td>0.562746</td>\n",
       "      <td>1.387614</td>\n",
       "      <td>-1.814780</td>\n",
       "      <td>-0.439519</td>\n",
       "      <td>-0.666038</td>\n",
       "      <td>-0.015537</td>\n",
       "      <td>0.299804</td>\n",
       "      <td>-1.486124</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008267</td>\n",
       "      <td>-0.049139</td>\n",
       "      <td>-0.298044</td>\n",
       "      <td>0.120919</td>\n",
       "      <td>0.069443</td>\n",
       "      <td>0.308891</td>\n",
       "      <td>-0.584395</td>\n",
       "      <td>-0.229966</td>\n",
       "      <td>0.123738</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152019</th>\n",
       "      <td>-0.293440</td>\n",
       "      <td>-3.705856</td>\n",
       "      <td>4.107873</td>\n",
       "      <td>-3.803656</td>\n",
       "      <td>1.710314</td>\n",
       "      <td>-3.582466</td>\n",
       "      <td>1.469729</td>\n",
       "      <td>-9.621560</td>\n",
       "      <td>-11.913105</td>\n",
       "      <td>-0.322297</td>\n",
       "      <td>...</td>\n",
       "      <td>3.639603</td>\n",
       "      <td>-5.498772</td>\n",
       "      <td>2.941475</td>\n",
       "      <td>0.916236</td>\n",
       "      <td>-0.255504</td>\n",
       "      <td>-0.183835</td>\n",
       "      <td>-0.584539</td>\n",
       "      <td>-0.315484</td>\n",
       "      <td>-0.097223</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6717</th>\n",
       "      <td>-0.293440</td>\n",
       "      <td>-1.813280</td>\n",
       "      <td>4.917851</td>\n",
       "      <td>-5.926130</td>\n",
       "      <td>5.701500</td>\n",
       "      <td>1.204393</td>\n",
       "      <td>-3.035138</td>\n",
       "      <td>-1.713402</td>\n",
       "      <td>0.561257</td>\n",
       "      <td>-3.796354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.576656</td>\n",
       "      <td>0.615642</td>\n",
       "      <td>-0.406427</td>\n",
       "      <td>-0.737018</td>\n",
       "      <td>-0.279642</td>\n",
       "      <td>1.106766</td>\n",
       "      <td>0.323885</td>\n",
       "      <td>0.894767</td>\n",
       "      <td>0.569519</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        scaled_amount        V1        V2        V3        V4        V5  \\\n",
       "210893      -0.273178  2.078547 -0.132505 -2.092574  0.118425  0.513681   \n",
       "79536       -0.293440 -0.264869  3.386140 -3.454997  4.367629  3.336060   \n",
       "143250      -0.028645 -1.731864  0.562746  1.387614 -1.814780 -0.439519   \n",
       "152019      -0.293440 -3.705856  4.107873 -3.803656  1.710314 -3.582466   \n",
       "6717        -0.293440 -1.813280  4.917851 -5.926130  5.701500  1.204393   \n",
       "\n",
       "              V6        V7         V8        V9  ...         V20       V21  \\\n",
       "210893 -0.852285  0.399962  -0.188309  0.196690  ...   -0.328672  0.125959   \n",
       "79536  -2.053918  0.256890  -2.957235 -2.855797  ...    0.482513 -1.394504   \n",
       "143250 -0.666038 -0.015537   0.299804 -1.486124  ...   -0.008267 -0.049139   \n",
       "152019  1.469729 -9.621560 -11.913105 -0.322297  ...    3.639603 -5.498772   \n",
       "6717   -3.035138 -1.713402   0.561257 -3.796354  ...    0.576656  0.615642   \n",
       "\n",
       "             V22       V23       V24       V25       V26       V27       V28  \\\n",
       "210893  0.410059  0.030725  0.741450  0.257435  0.783731 -0.140019 -0.094269   \n",
       "79536  -0.166029 -1.452081 -0.251815  1.243461  0.452787  0.132218  0.424599   \n",
       "143250 -0.298044  0.120919  0.069443  0.308891 -0.584395 -0.229966  0.123738   \n",
       "152019  2.941475  0.916236 -0.255504 -0.183835 -0.584539 -0.315484 -0.097223   \n",
       "6717   -0.406427 -0.737018 -0.279642  1.106766  0.323885  0.894767  0.569519   \n",
       "\n",
       "        Class  \n",
       "210893      0  \n",
       "79536       1  \n",
       "143250      0  \n",
       "152019      1  \n",
       "6717        1  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Membuat sub sample 50:50\n",
    "\n",
    "#Karena ada 492 kasus transaksi fraud, maka akan diambil secara acak 492 kasus transaksi non-fraud\n",
    "\n",
    "#merandom dataset\n",
    "data = data.sample(frac=1)\n",
    "\n",
    "#mengambil seluruh data fraud dan 492 kasus data non-fraud\n",
    "fraud_data = data.loc[data['Class'] == 1]\n",
    "nonFraud_data = data.loc[data['Class']==0][:492]\n",
    "\n",
    "#menggabungkan data fraud dan non-fraud\n",
    "normal_distributed_data = pd.concat([fraud_data, nonFraud_data])\n",
    "\n",
    "#membuat dataframe baru yang berisi sub sample 50:50 kemudian mengacak baris pada data baru\n",
    "subSample_data = normal_distributed_data.sample(frac=1, random_state=42)\n",
    "# subSample_data.shape\n",
    "subSample_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Undersampling sebelum melakukan cross-validation\n",
    "\n",
    "X = subSample_data.drop('Class', axis=1)\n",
    "y = subSample_data['Class']\n",
    "\n",
    "#Memecah dataframe baru menjadi training dan test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)\n",
    "#test_size = perbandingan pembagian training test dengan testing set\n",
    "#total data = 984\n",
    "#training set = 787\n",
    "#testing set = 197\n",
    "\n",
    "#mengubah ke array agar lebih mudah untuk algoritma klasifikasi\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values\n",
    "\n",
    "#X_train.shape\n",
    "#X_test.shape\n",
    "#y_train.shape\n",
    "#y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training logistic regression untuk under sampling memakan waktu: 0.197131 detik\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Menerapkan algoritma klasifikasi Logistic Regression\n",
    "\n",
    "classifiers = {\"LogReg\" : LogisticRegression()}\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "t0 = time.time()\n",
    "for key, classifier in classifiers.items():\n",
    "    classifier.fit(X_train, y_train)\n",
    "    training_score = cross_val_score(classifier, X_train, y_train,\n",
    "                                    cv=5)\n",
    "#     print(\"Classifiers: \", classifier.__class__.__name__, \"training skor akurasinya\", round(training_score.mean(),2)*100,\n",
    " #        \"%\")\n",
    "t1 = time.time()\n",
    "\n",
    "print ('Training logistic regression untuk under sampling memakan waktu: {:3f} detik' .format(t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# t0 = time.time()\n",
    "# Logistic Regression \n",
    "log_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "\n",
    "\n",
    "\n",
    "grid_log_reg = GridSearchCV(LogisticRegression(), log_reg_params)\n",
    "grid_log_reg.fit(X_train, y_train)\n",
    "# We automatically get the logistic regression with the best parameters.\n",
    "log_reg = grid_log_reg.best_estimator_\n",
    "\n",
    "print (log_reg)\n",
    "\n",
    "# log_reg_score = cross_val_score(log_reg, X_train, y_train, cv=5)\n",
    "# print('Logistic Regression Cross Validation Score: ', round(log_reg_score.mean() * 100, 2).astype(str) + '%')\n",
    "# t1 = time.time()\n",
    "# print ('Cross Validation pada Logistic Regression untuk sub sample 50:50 memakan waktu: {:3f} detik' .format(t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:  0.9787925796651802\n",
      "Menghitung skor AUC Logistic Regression untuk under sampling memakan waktu: 0.076054 detik\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# Create a DataFrame with all the scores and the classifiers names.\n",
    "t0 = time.time()\n",
    "log_reg_pred = cross_val_predict(log_reg, X_train, y_train, cv=5,\n",
    "                             method=\"decision_function\")\n",
    "print('Logistic Regression: ', roc_auc_score(y_train, log_reg_pred))\n",
    "t1 = time.time()\n",
    "\n",
    "print ('Menghitung skor AUC Logistic Regression untuk under sampling memakan waktu: {:3f} detik' .format(t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score\n",
    "\n",
    "# t0= time.time()\n",
    "# y_pred = log_reg.predict(X_train)\n",
    "\n",
    "# print ('Classification Report Logistic Regression:')\n",
    "# print('Recall Score: {:.2f}'.format(recall_score(y_train, y_pred)))\n",
    "# print('Precision Score: {:.2f}'.format(precision_score(y_train, y_pred)))\n",
    "# print('F1 Score: {:.2f}'.format(f1_score(y_train, y_pred)))\n",
    "# print('Accuracy Score: {:.2f}'.format(accuracy_score(y_train, y_pred)))\n",
    "\n",
    "# t1= time.time()\n",
    "# print ('Menghitung recall, precision, f1, dan akurasi pada Logistic Regression untuk down sampling memerlukan waktu: {:3f} detik' .format(t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284807, 30)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mengambil seluruh data fraud dan non-fraud\n",
    "fraud_data_all = data2.loc[data2['Class'] == 1]\n",
    "nonFraud_data_all = data2.loc[data2['Class']==0]\n",
    "\n",
    "#menggabungkan data fraud dan non-fraud\n",
    "normal_distributed_data_all = pd.concat([fraud_data_all, nonFraud_data_all])\n",
    "normal_distributed_data_all.shape\n",
    "#normal_distributed_data_all.head()\n",
    "# data2.head()\n",
    "#fraud_data_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = normal_distributed_data_all.drop('Class', axis=1)\n",
    "y2 = normal_distributed_data_all['Class']\n",
    "\n",
    "#Memecah dataframe baru menjadi training dan test set\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size=0.2, random_state=42)\n",
    "\n",
    "#mengubah ke array agar lebih mudah untuk algoritma klasifikasi\n",
    "X_train2 = X_train2.values\n",
    "X_test2 = X_test2.values\n",
    "y_train2 = y_train2.values\n",
    "y_test2 = y_test2.values\n",
    "\n",
    "#y_train2.shape\n",
    "#y_test2.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training logistic regression untuk seluruh data memakan waktu: 76.932954 detik\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "for key, classifier in classifiers.items():\n",
    "    classifier.fit(X_train2, y_train2)\n",
    "    training_score = cross_val_score(classifier, X_train2, y_train2,\n",
    "                                    cv=5)\n",
    "#     print(\"Classifiers: \", classifier.__class__.__name__, \"training skor akurasinya\", round(training_score.mean(),2)*100,\n",
    "    #     \"%\")\n",
    "t1 = time.time()\n",
    "\n",
    "print ('Training logistic regression untuk seluruh data memakan waktu: {:3f} detik' .format(t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "# t0 = time.time()\n",
    "# Logistic Regression \n",
    "log_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "\n",
    "\n",
    "\n",
    "grid_log_reg2 = GridSearchCV(LogisticRegression(), log_reg_params)\n",
    "grid_log_reg2.fit(X_train2, y_train2)\n",
    "# We automatically get the logistic regression with the best parameters.\n",
    "log_reg2 = grid_log_reg2.best_estimator_\n",
    "\n",
    "print (log_reg2)\n",
    "# log_reg_score2 = cross_val_score(log_reg2, X_train2, y_train2, cv=5)\n",
    "# print('Logistic Regression Cross Validation Score: ', round(log_reg_score2.mean() * 100, 2).astype(str) + '%')\n",
    "# t1 = time.time()\n",
    "# print ('Cross Validation pada Logistic Regression untuk seluruh data memakan waktu: {:3f} detik' .format(t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:  0.960492719624437\n",
      "Menghitung skor AUC Logistic Regression untuk seluruh data memakan waktu: 28.926193 detik\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "log_reg_pred2 = cross_val_predict(log_reg2, X_train2, y_train2, cv=5,\n",
    "                             method=\"decision_function\")\n",
    "print('Logistic Regression: ', roc_auc_score(y_train2, log_reg_pred2))\n",
    "t1 = time.time()\n",
    "\n",
    "print ('Menghitung skor AUC Logistic Regression untuk seluruh data memakan waktu: {:3f} detik' .format(t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t0= time.time()\n",
    "# y_pred2 = log_reg2.predict(X_train2)\n",
    "\n",
    "# print ('Classification Report Logistic Regression:')\n",
    "# print('Recall Score: {:.2f}'.format(recall_score(y_train2, y_pred2)))\n",
    "# print('Precision Score: {:.2f}'.format(precision_score(y_train2, y_pred2)))\n",
    "# print('F1 Score: {:.2f}'.format(f1_score(y_train2, y_pred2)))\n",
    "# print('Accuracy Score: {:.2f}'.format(accuracy_score(y_train2, y_pred2)))\n",
    "\n",
    "# t1= time.time()\n",
    "# print ('Menghitung recall, precision, f1, dan akurasi pada Logistic Regression untuk seluruh data memerlukan waktu: {:3f} detik' .format(t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#Menerapkan confusion matrix pada testing set\n",
    "y_pred_log_reg = log_reg.predict(X_test2)\n",
    "\n",
    "\n",
    "#log\n",
    "#mengganti testing setnya dengan testing set untuk data keseluruhan\n",
    "log_reg_cf = confusion_matrix(y_test2, y_pred_log_reg)\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(5,4))\n",
    "# #classNames = ['Positive', 'Negative']\n",
    "# sns.heatmap(log_reg_cf, annot=True, cmap=plt.cm.inferno)\n",
    "# plt.title(\"Confusion Matrix dengan Logistic Regression \\n Pada Data Down Sampling\", fontsize=14)\n",
    "# #tick_marks = np.arrange(len(classNames))\n",
    "# plt.xlabel('Predicted', fontsize=19)\n",
    "# plt.ylabel('Actual', fontsize=19)\n",
    "\n",
    "# # s = [['TN','FP'], ['FN', 'TP']]\n",
    "# # for i in range(2):\n",
    "# #     for j in range(2):\n",
    "# #         plt.text(j,i, str(s[i][j])+\" = \"+str(log_reg_cf[i][j]))\n",
    "\n",
    "# #TN = kiri atas -->Non Fraud\n",
    "# #FP = kanan atas\n",
    "# #FN = kiri bawah\n",
    "# #TP = kanan bawah -->Fraud\n",
    "\n",
    "# ax.set_xticklabels(['Negative', 'Positive'], fontsize=14, rotation=360)\n",
    "# ax.set_yticklabels(['Negative', 'Positive'], fontsize=14, rotation=360)\n",
    "\n",
    "# plt.show()\n",
    "import itertools\n",
    "\n",
    "# Create a confusion matrix\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"black\" if cm[i, j] > thresh else \"white\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "labels = ['No Fraud', 'Fraud']\n",
    "\n",
    "fig = plt.figure(figsize=(5,4))\n",
    "\n",
    "plot_confusion_matrix(log_reg_cf, labels, title=\"Confusion Matrix Logistic Regression \\n Under Sampling\", cmap=plt.cm.inferno)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred_log_reg2 = log_reg2.predict(X_test2)\n",
    "\n",
    "log_reg_cf2 = confusion_matrix(y_test2, y_pred_log_reg2)\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(5,4))\n",
    "# #classNames = ['Positive', 'Negative']\n",
    "# sns.heatmap(log_reg_cf2, annot=True, cmap=plt.cm.inferno)\n",
    "# plt.title(\"Confusion Matrix dengan Logistic Regression \\n Pada Seluruh Data\", fontsize=14)\n",
    "# #tick_marks = np.arrange(len(classNames))\n",
    "# plt.xlabel('Predicted', fontsize=19)\n",
    "# plt.ylabel('Actual', fontsize=19)\n",
    "\n",
    "# #TN = kiri atas -->Non Fraud\n",
    "# #FP = kanan atas\n",
    "# #FN = kiri bawah\n",
    "# #TP = kanan bawah -->Fraud\n",
    "\n",
    "# ax.set_xticklabels(['Negative', 'Positive'], fontsize=14, rotation=360)\n",
    "# ax.set_yticklabels(['Negative', 'Positive'], fontsize=14, rotation=360)\n",
    "fig = plt.figure(figsize=(5,4))\n",
    "\n",
    "plot_confusion_matrix(log_reg_cf2, labels, title=\"Confusion Matrix dengan Logistic Regression \\n Pada Seluruh Data\", \n",
    "                      cmap=plt.cm.inferno)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression dengan Down Sampling:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.97     56866\n",
      "           1       0.02      0.90      0.04        96\n",
      "\n",
      "   micro avg       0.94      0.94      0.94     56962\n",
      "   macro avg       0.51      0.92      0.51     56962\n",
      "weighted avg       1.00      0.94      0.97     56962\n",
      "\n",
      "Logistic Regression dengan Seluruh Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56866\n",
      "           1       0.88      0.51      0.64        96\n",
      "\n",
      "   micro avg       1.00      1.00      1.00     56962\n",
      "   macro avg       0.94      0.76      0.82     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Laporan Hasil Klasifikasi\n",
    "\n",
    "print('Logistic Regression dengan Down Sampling:')\n",
    "print(classification_report(y_test2, y_pred_log_reg))\n",
    "\n",
    "\n",
    "print('Logistic Regression dengan Seluruh Data:')\n",
    "print(classification_report(y_test2, y_pred_log_reg2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geometrcial mean under sampling:  0.9155366539259276\n",
      "Geometrical mean seluruh data:  0.7143905347997301\n"
     ]
    }
   ],
   "source": [
    "G_mean_under_sampling = geometric_mean_score (y_test2, y_pred_log_reg)\n",
    "G_mean_all_data = geometric_mean_score (y_test2, y_pred_log_reg2)\n",
    "\n",
    "print (\"Geometrcial mean under sampling: \", G_mean_under_sampling)\n",
    "print (\"Geometrical mean seluruh data: \", G_mean_all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "'Python Interactive'",
   "language": "python",
   "name": "1875d08a-4cae-4e1b-b42a-38e4d5817e1d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
