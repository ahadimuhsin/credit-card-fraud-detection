{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "from imblearn.under_sampling import EditedNearestNeighbours, RepeatedEditedNearestNeighbours, ClusterCentroids, NearMiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
      "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
      "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',\n",
      "       'Class'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('F:\\Kuliah\\Semester 7\\Data Mining\\Data Dow Jones\\creditcard.csv', sep=',')\n",
    "#data diambil dari kaggle.com/mlg-ulb/creditcardfraud\n",
    "\n",
    "data2 = data\n",
    "\n",
    "print (data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Time         V1         V2        V3        V4        V5  \\\n",
      "0            0.0  -1.359807  -0.072781  2.536347  1.378155 -0.338321   \n",
      "1            0.0   1.191857   0.266151  0.166480  0.448154  0.060018   \n",
      "2            1.0  -1.358354  -1.340163  1.773209  0.379780 -0.503198   \n",
      "3            1.0  -0.966272  -0.185226  1.792993 -0.863291 -0.010309   \n",
      "4            2.0  -1.158233   0.877737  1.548718  0.403034 -0.407193   \n",
      "5            2.0  -0.425966   0.960523  1.141109 -0.168252  0.420987   \n",
      "6            4.0   1.229658   0.141004  0.045371  1.202613  0.191881   \n",
      "7            7.0  -0.644269   1.417964  1.074380 -0.492199  0.948934   \n",
      "8            7.0  -0.894286   0.286157 -0.113192 -0.271526  2.669599   \n",
      "9            9.0  -0.338262   1.119593  1.044367 -0.222187  0.499361   \n",
      "10          10.0   1.449044  -1.176339  0.913860 -1.375667 -1.971383   \n",
      "11          10.0   0.384978   0.616109 -0.874300 -0.094019  2.924584   \n",
      "12          10.0   1.249999  -1.221637  0.383930 -1.234899 -1.485419   \n",
      "13          11.0   1.069374   0.287722  0.828613  2.712520 -0.178398   \n",
      "14          12.0  -2.791855  -0.327771  1.641750  1.767473 -0.136588   \n",
      "15          12.0  -0.752417   0.345485  2.057323 -1.468643 -1.158394   \n",
      "16          12.0   1.103215  -0.040296  1.267332  1.289091 -0.735997   \n",
      "17          13.0  -0.436905   0.918966  0.924591 -0.727219  0.915679   \n",
      "18          14.0  -5.401258  -5.450148  1.186305  1.736239  3.049106   \n",
      "19          15.0   1.492936  -1.029346  0.454795 -1.438026 -1.555434   \n",
      "20          16.0   0.694885  -1.361819  1.029221  0.834159 -1.191209   \n",
      "21          17.0   0.962496   0.328461 -0.171479  2.109204  1.129566   \n",
      "22          18.0   1.166616   0.502120 -0.067300  2.261569  0.428804   \n",
      "23          18.0   0.247491   0.277666  1.185471 -0.092603 -1.314394   \n",
      "24          22.0  -1.946525  -0.044901 -0.405570 -1.013057  2.941968   \n",
      "25          22.0  -2.074295  -0.121482  1.322021  0.410008  0.295198   \n",
      "26          23.0   1.173285   0.353498  0.283905  1.133563 -0.172577   \n",
      "27          23.0   1.322707  -0.174041  0.434555  0.576038 -0.836758   \n",
      "28          23.0  -0.414289   0.905437  1.727453  1.473471  0.007443   \n",
      "29          23.0   1.059387  -0.175319  1.266130  1.186110 -0.786002   \n",
      "...          ...        ...        ...       ...       ...       ...   \n",
      "284777  172764.0   2.079137  -0.028723 -1.343392  0.358000 -0.045791   \n",
      "284778  172764.0  -0.764523   0.588379 -0.907599 -0.418847  0.901528   \n",
      "284779  172766.0   1.975178  -0.616244 -2.628295 -0.406246  2.327804   \n",
      "284780  172766.0  -1.727503   1.108356  2.219561  1.148583 -0.884199   \n",
      "284781  172766.0  -1.139015  -0.155510  1.894478 -1.138957  1.451777   \n",
      "284782  172767.0  -0.268061   2.540315 -1.400915  4.846661  0.639105   \n",
      "284783  172768.0  -1.796092   1.929178 -2.828417 -1.689844  2.199572   \n",
      "284784  172768.0  -0.669662   0.923769 -1.543167 -1.560729  2.833960   \n",
      "284785  172768.0   0.032887   0.545338 -1.185844 -1.729828  2.932315   \n",
      "284786  172768.0  -2.076175   2.142238 -2.522704 -1.888063  1.982785   \n",
      "284787  172769.0  -1.029719  -1.110670 -0.636179 -0.840816  2.424360   \n",
      "284788  172770.0   2.007418  -0.280235 -0.208113  0.335261 -0.715798   \n",
      "284789  172770.0  -0.446951   1.302212 -0.168583  0.981577  0.578957   \n",
      "284790  172771.0  -0.515513   0.971950 -1.014580 -0.677037  0.912430   \n",
      "284791  172774.0  -0.863506   0.874701  0.420358 -0.530365  0.356561   \n",
      "284792  172774.0  -0.724123   1.485216 -1.132218 -0.607190  0.709499   \n",
      "284793  172775.0   1.971002  -0.699067 -1.697541 -0.617643  1.718797   \n",
      "284794  172777.0  -1.266580  -0.400461  0.956221 -0.723919  1.531993   \n",
      "284795  172778.0 -12.516732  10.187818 -8.476671 -2.510473 -4.586669   \n",
      "284796  172780.0   1.884849  -0.143540 -0.999943  1.506772 -0.035300   \n",
      "284797  172782.0  -0.241923   0.712247  0.399806 -0.463406  0.244531   \n",
      "284798  172782.0   0.219529   0.881246 -0.635891  0.960928 -0.152971   \n",
      "284799  172783.0  -1.775135  -0.004235  1.189786  0.331096  1.196063   \n",
      "284800  172784.0   2.039560  -0.175233 -1.196825  0.234580 -0.008713   \n",
      "284801  172785.0   0.120316   0.931005 -0.546012 -0.745097  1.130314   \n",
      "284802  172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473   \n",
      "284803  172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229   \n",
      "284804  172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515   \n",
      "284805  172788.0  -0.240440   0.530483  0.702510  0.689799 -0.377961   \n",
      "284806  172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546   \n",
      "\n",
      "              V6        V7        V8        V9  ...       V21       V22  \\\n",
      "0       0.462388  0.239599  0.098698  0.363787  ... -0.018307  0.277838   \n",
      "1      -0.082361 -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672   \n",
      "2       1.800499  0.791461  0.247676 -1.514654  ...  0.247998  0.771679   \n",
      "3       1.247203  0.237609  0.377436 -1.387024  ... -0.108300  0.005274   \n",
      "4       0.095921  0.592941 -0.270533  0.817739  ... -0.009431  0.798278   \n",
      "5      -0.029728  0.476201  0.260314 -0.568671  ... -0.208254 -0.559825   \n",
      "6       0.272708 -0.005159  0.081213  0.464960  ... -0.167716 -0.270710   \n",
      "7       0.428118  1.120631 -3.807864  0.615375  ...  1.943465 -1.015455   \n",
      "8       3.721818  0.370145  0.851084 -0.392048  ... -0.073425 -0.268092   \n",
      "9      -0.246761  0.651583  0.069539 -0.736727  ... -0.246914 -0.633753   \n",
      "10     -0.629152 -1.423236  0.048456 -1.720408  ... -0.009302  0.313894   \n",
      "11      3.317027  0.470455  0.538247 -0.558895  ...  0.049924  0.238422   \n",
      "12     -0.753230 -0.689405 -0.227487 -2.094011  ... -0.231809 -0.483285   \n",
      "13      0.337544 -0.096717  0.115982 -0.221083  ... -0.036876  0.074412   \n",
      "14      0.807596 -0.422911 -1.907107  0.755713  ...  1.151663  0.222182   \n",
      "15     -0.077850 -0.608581  0.003603 -0.436167  ...  0.499625  1.353650   \n",
      "16      0.288069 -0.586057  0.189380  0.782333  ... -0.024612  0.196002   \n",
      "17     -0.127867  0.707642  0.087962 -0.665271  ... -0.194796 -0.672638   \n",
      "18     -1.763406 -1.559738  0.160842  1.233090  ... -0.503600  0.984460   \n",
      "19     -0.720961 -1.080664 -0.053127 -1.978682  ... -0.177650 -0.175074   \n",
      "20      1.309109 -0.878586  0.445290 -0.446196  ... -0.295583 -0.571955   \n",
      "21      1.696038  0.107712  0.521502 -1.191311  ...  0.143997  0.402492   \n",
      "22      0.089474  0.241147  0.138082 -0.989162  ...  0.018702 -0.061972   \n",
      "23     -0.150116 -0.946365 -1.617935  1.544071  ...  1.650180  0.200454   \n",
      "24      2.955053 -0.063063  0.855546  0.049967  ... -0.579526 -0.799229   \n",
      "25     -0.959537  0.543985 -0.104627  0.475664  ... -0.403639 -0.227404   \n",
      "26     -0.916054  0.369025 -0.327260 -0.246651  ...  0.067003  0.227812   \n",
      "27     -0.831083 -0.264905 -0.220982 -1.071425  ... -0.284376 -0.323357   \n",
      "28     -0.200331  0.740228 -0.029247 -0.593392  ...  0.077237  0.457331   \n",
      "29      0.578435 -0.767084  0.401046  0.699500  ...  0.013676  0.213734   \n",
      "...          ...       ...       ...       ...  ...       ...       ...   \n",
      "284777 -1.345452  0.227476 -0.378355  0.665911  ...  0.235758  0.829758   \n",
      "284778 -0.760802  0.758545  0.414698 -0.730854  ...  0.003530 -0.431876   \n",
      "284779  3.664740 -0.533297  0.842937  1.128798  ...  0.086043  0.543613   \n",
      "284780  0.793083 -0.527298  0.866429  0.853819  ... -0.094708  0.236818   \n",
      "284781  0.093598  0.191353  0.092211 -0.062621  ... -0.191027 -0.631658   \n",
      "284782  0.186479 -0.045911  0.936448 -2.419986  ... -0.263889 -0.857904   \n",
      "284783  3.123732 -0.270714  1.657495  0.465804  ...  0.271170  1.145750   \n",
      "284784  3.240843  0.181576  1.282746 -0.893890  ...  0.183856  0.202670   \n",
      "284785  3.401529  0.337434  0.925377 -0.165663  ... -0.266113 -0.716336   \n",
      "284786  3.732950 -1.217430 -0.536644  0.272867  ...  2.016666 -1.588269   \n",
      "284787 -2.956733  0.283610 -0.332656 -0.247488  ...  0.353722  0.488487   \n",
      "284788 -0.751373 -0.458972 -0.140140  0.959971  ... -0.208260 -0.430347   \n",
      "284789 -0.605641  1.253430 -1.042610 -0.417116  ...  0.851800  0.305268   \n",
      "284790 -0.316187  0.396137  0.532364 -0.224606  ... -0.280302 -0.849919   \n",
      "284791 -1.046238  0.757051  0.230473 -0.506856  ... -0.108846 -0.480820   \n",
      "284792 -0.482638  0.548393  0.343003 -0.226323  ...  0.414621  1.307511   \n",
      "284793  3.911336 -1.259306  1.056209  1.315006  ...  0.188758  0.694418   \n",
      "284794 -1.788600  0.314741  0.004704  0.013857  ... -0.157831 -0.883365   \n",
      "284795 -1.394465 -3.632516  5.498583  4.893089  ... -0.944759 -1.565026   \n",
      "284796 -0.613638  0.190241 -0.249058  0.666458  ...  0.144008  0.634646   \n",
      "284797 -1.343668  0.929369 -0.206210  0.106234  ... -0.228876 -0.514376   \n",
      "284798 -1.014307  0.427126  0.121340 -0.285670  ...  0.099936  0.337120   \n",
      "284799  5.519980 -1.518185  2.080825  1.159498  ...  0.103302  0.654850   \n",
      "284800 -0.726571  0.017050 -0.118228  0.435402  ... -0.268048 -0.717211   \n",
      "284801 -0.235973  0.812722  0.115093 -0.204064  ... -0.314205 -0.808520   \n",
      "284802 -2.606837 -4.918215  7.305334  1.914428  ...  0.213454  0.111864   \n",
      "284803  1.058415  0.024330  0.294869  0.584800  ...  0.214205  0.924384   \n",
      "284804  3.031260 -0.296827  0.708417  0.432454  ...  0.232045  0.578229   \n",
      "284805  0.623708 -0.686180  0.679145  0.392087  ...  0.265245  0.800049   \n",
      "284806 -0.649617  1.577006 -0.414650  0.486180  ...  0.261057  0.643078   \n",
      "\n",
      "             V23       V24       V25       V26       V27       V28  Amount  \\\n",
      "0      -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62   \n",
      "1       0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69   \n",
      "2       0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66   \n",
      "3      -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50   \n",
      "4      -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   69.99   \n",
      "5      -0.026398 -0.371427 -0.232794  0.105915  0.253844  0.081080    3.67   \n",
      "6      -0.154104 -0.780055  0.750137 -0.257237  0.034507  0.005168    4.99   \n",
      "7       0.057504 -0.649709 -0.415267 -0.051634 -1.206921 -1.085339   40.80   \n",
      "8      -0.204233  1.011592  0.373205 -0.384157  0.011747  0.142404   93.20   \n",
      "9      -0.120794 -0.385050 -0.069733  0.094199  0.246219  0.083076    3.68   \n",
      "10      0.027740  0.500512  0.251367 -0.129478  0.042850  0.016253    7.80   \n",
      "11      0.009130  0.996710 -0.767315 -0.492208  0.042472 -0.054337    9.99   \n",
      "12      0.084668  0.392831  0.161135 -0.354990  0.026416  0.042422  121.50   \n",
      "13     -0.071407  0.104744  0.548265  0.104094  0.021491  0.021293   27.50   \n",
      "14      1.020586  0.028317 -0.232746 -0.235557 -0.164778 -0.030154   58.80   \n",
      "15     -0.256573 -0.065084 -0.039124 -0.087086 -0.180998  0.129394   15.99   \n",
      "16      0.013802  0.103758  0.364298 -0.382261  0.092809  0.037051   12.99   \n",
      "17     -0.156858 -0.888386 -0.342413 -0.049027  0.079692  0.131024    0.89   \n",
      "18      2.458589  0.042119 -0.481631 -0.621272  0.392053  0.949594   46.80   \n",
      "19      0.040002  0.295814  0.332931 -0.220385  0.022298  0.007602    5.00   \n",
      "20     -0.050881 -0.304215  0.072001 -0.422234  0.086553  0.063499  231.71   \n",
      "21     -0.048508 -1.371866  0.390814  0.199964  0.016371 -0.014605   34.09   \n",
      "22     -0.103855 -0.370415  0.603200  0.108556 -0.040521 -0.011418    2.28   \n",
      "23     -0.185353  0.423073  0.820591 -0.227632  0.336634  0.250475   22.75   \n",
      "24      0.870300  0.983421  0.321201  0.149650  0.707519  0.014600    0.89   \n",
      "25      0.742435  0.398535  0.249212  0.274404  0.359969  0.243232   26.43   \n",
      "26     -0.150487  0.435045  0.724825 -0.337082  0.016368  0.030041   41.88   \n",
      "27     -0.037710  0.347151  0.559639 -0.280158  0.042335  0.028822   16.00   \n",
      "28     -0.038500  0.642522 -0.183891 -0.277464  0.182687  0.152665   33.00   \n",
      "29      0.014462  0.002951  0.294638 -0.395070  0.081461  0.024220   12.99   \n",
      "...          ...       ...       ...       ...       ...       ...     ...   \n",
      "284777 -0.002063  0.001344  0.262183 -0.105327 -0.022363 -0.060283    1.00   \n",
      "284778  0.141759  0.587119 -0.200998  0.267337 -0.152951 -0.065285   80.00   \n",
      "284779 -0.032129  0.768379  0.477688 -0.031833  0.014151 -0.066542   25.00   \n",
      "284780 -0.204280  1.158185  0.627801 -0.399981  0.510818  0.233265   30.00   \n",
      "284781 -0.147249  0.212931  0.354257 -0.241068 -0.161717 -0.149188   13.00   \n",
      "284782  0.235172 -0.681794 -0.668894  0.044657 -0.066751 -0.072447   12.82   \n",
      "284783  0.084783  0.721269 -0.529906 -0.240117  0.129126 -0.080620   11.46   \n",
      "284784 -0.373023  0.651122  1.073823  0.844590 -0.286676 -0.187719   40.00   \n",
      "284785  0.108519  0.688519 -0.460220  0.161939  0.265368  0.090245    1.79   \n",
      "284786  0.588482  0.632444 -0.201064  0.199251  0.438657  0.172923    8.95   \n",
      "284787  0.293632  0.107812 -0.935586  1.138216  0.025271  0.255347    9.99   \n",
      "284788  0.416765  0.064819 -0.608337  0.268436 -0.028069 -0.041367    3.99   \n",
      "284789 -0.148093 -0.038712  0.010209 -0.362666  0.503092  0.229921   60.50   \n",
      "284790  0.300245  0.000607 -0.376379  0.128660 -0.015205 -0.021486    9.81   \n",
      "284791 -0.074513 -0.003988 -0.113149  0.280378 -0.077310  0.023079   20.32   \n",
      "284792 -0.059545  0.242669 -0.665424 -0.269869 -0.170579 -0.030692    3.99   \n",
      "284793  0.163002  0.726365 -0.058282 -0.191813  0.061858 -0.043716    4.99   \n",
      "284794  0.088485 -0.076790 -0.095833  0.132720 -0.028468  0.126494    0.89   \n",
      "284795  0.890675 -1.253276  1.786717  0.320763  2.090712  1.232864    9.87   \n",
      "284796 -0.042114 -0.053206  0.316403 -0.461441  0.018265 -0.041068   60.00   \n",
      "284797  0.279598  0.371441 -0.559238  0.113144  0.131507  0.081265    5.49   \n",
      "284798  0.251791  0.057688 -1.508368  0.144023  0.181205  0.215243   24.05   \n",
      "284799 -0.348929  0.745323  0.704545 -0.127579  0.454379  0.130308   79.99   \n",
      "284800  0.297930 -0.359769 -0.315610  0.201114 -0.080826 -0.075071    2.68   \n",
      "284801  0.050343  0.102800 -0.435870  0.124079  0.217940  0.068803    2.69   \n",
      "284802  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731    0.77   \n",
      "284803  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   24.79   \n",
      "284804 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   67.88   \n",
      "284805 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   10.00   \n",
      "284806  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649  217.00   \n",
      "\n",
      "        Class  \n",
      "0           0  \n",
      "1           0  \n",
      "2           0  \n",
      "3           0  \n",
      "4           0  \n",
      "5           0  \n",
      "6           0  \n",
      "7           0  \n",
      "8           0  \n",
      "9           0  \n",
      "10          0  \n",
      "11          0  \n",
      "12          0  \n",
      "13          0  \n",
      "14          0  \n",
      "15          0  \n",
      "16          0  \n",
      "17          0  \n",
      "18          0  \n",
      "19          0  \n",
      "20          0  \n",
      "21          0  \n",
      "22          0  \n",
      "23          0  \n",
      "24          0  \n",
      "25          0  \n",
      "26          0  \n",
      "27          0  \n",
      "28          0  \n",
      "29          0  \n",
      "...       ...  \n",
      "284777      0  \n",
      "284778      0  \n",
      "284779      0  \n",
      "284780      0  \n",
      "284781      0  \n",
      "284782      0  \n",
      "284783      0  \n",
      "284784      0  \n",
      "284785      0  \n",
      "284786      0  \n",
      "284787      0  \n",
      "284788      0  \n",
      "284789      0  \n",
      "284790      0  \n",
      "284791      0  \n",
      "284792      0  \n",
      "284793      0  \n",
      "284794      0  \n",
      "284795      0  \n",
      "284796      0  \n",
      "284797      0  \n",
      "284798      0  \n",
      "284799      0  \n",
      "284800      0  \n",
      "284801      0  \n",
      "284802      0  \n",
      "284803      0  \n",
      "284804      0  \n",
      "284805      0  \n",
      "284806      0  \n",
      "\n",
      "[284807 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "#print seluruh data\n",
    "print (data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284807, 31)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>94813.859575</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>88.349619</td>\n",
       "      <td>0.001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>47488.145955</td>\n",
       "      <td>1.958696</td>\n",
       "      <td>1.651309</td>\n",
       "      <td>1.516255</td>\n",
       "      <td>1.415869</td>\n",
       "      <td>1.380247</td>\n",
       "      <td>1.332271</td>\n",
       "      <td>1.237094</td>\n",
       "      <td>1.194353</td>\n",
       "      <td>1.098632</td>\n",
       "      <td>...</td>\n",
       "      <td>0.734524</td>\n",
       "      <td>0.725702</td>\n",
       "      <td>0.624460</td>\n",
       "      <td>0.605647</td>\n",
       "      <td>0.521278</td>\n",
       "      <td>0.482227</td>\n",
       "      <td>0.403632</td>\n",
       "      <td>0.330083</td>\n",
       "      <td>250.120109</td>\n",
       "      <td>0.041527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-56.407510</td>\n",
       "      <td>-72.715728</td>\n",
       "      <td>-48.325589</td>\n",
       "      <td>-5.683171</td>\n",
       "      <td>-113.743307</td>\n",
       "      <td>-26.160506</td>\n",
       "      <td>-43.557242</td>\n",
       "      <td>-73.216718</td>\n",
       "      <td>-13.434066</td>\n",
       "      <td>...</td>\n",
       "      <td>-34.830382</td>\n",
       "      <td>-10.933144</td>\n",
       "      <td>-44.807735</td>\n",
       "      <td>-2.836627</td>\n",
       "      <td>-10.295397</td>\n",
       "      <td>-2.604551</td>\n",
       "      <td>-22.565679</td>\n",
       "      <td>-15.430084</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>54201.500000</td>\n",
       "      <td>-0.920373</td>\n",
       "      <td>-0.598550</td>\n",
       "      <td>-0.890365</td>\n",
       "      <td>-0.848640</td>\n",
       "      <td>-0.691597</td>\n",
       "      <td>-0.768296</td>\n",
       "      <td>-0.554076</td>\n",
       "      <td>-0.208630</td>\n",
       "      <td>-0.643098</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.228395</td>\n",
       "      <td>-0.542350</td>\n",
       "      <td>-0.161846</td>\n",
       "      <td>-0.354586</td>\n",
       "      <td>-0.317145</td>\n",
       "      <td>-0.326984</td>\n",
       "      <td>-0.070840</td>\n",
       "      <td>-0.052960</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>84692.000000</td>\n",
       "      <td>0.018109</td>\n",
       "      <td>0.065486</td>\n",
       "      <td>0.179846</td>\n",
       "      <td>-0.019847</td>\n",
       "      <td>-0.054336</td>\n",
       "      <td>-0.274187</td>\n",
       "      <td>0.040103</td>\n",
       "      <td>0.022358</td>\n",
       "      <td>-0.051429</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029450</td>\n",
       "      <td>0.006782</td>\n",
       "      <td>-0.011193</td>\n",
       "      <td>0.040976</td>\n",
       "      <td>0.016594</td>\n",
       "      <td>-0.052139</td>\n",
       "      <td>0.001342</td>\n",
       "      <td>0.011244</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>139320.500000</td>\n",
       "      <td>1.315642</td>\n",
       "      <td>0.803724</td>\n",
       "      <td>1.027196</td>\n",
       "      <td>0.743341</td>\n",
       "      <td>0.611926</td>\n",
       "      <td>0.398565</td>\n",
       "      <td>0.570436</td>\n",
       "      <td>0.327346</td>\n",
       "      <td>0.597139</td>\n",
       "      <td>...</td>\n",
       "      <td>0.186377</td>\n",
       "      <td>0.528554</td>\n",
       "      <td>0.147642</td>\n",
       "      <td>0.439527</td>\n",
       "      <td>0.350716</td>\n",
       "      <td>0.240952</td>\n",
       "      <td>0.091045</td>\n",
       "      <td>0.078280</td>\n",
       "      <td>77.165000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>172792.000000</td>\n",
       "      <td>2.454930</td>\n",
       "      <td>22.057729</td>\n",
       "      <td>9.382558</td>\n",
       "      <td>16.875344</td>\n",
       "      <td>34.801666</td>\n",
       "      <td>73.301626</td>\n",
       "      <td>120.589494</td>\n",
       "      <td>20.007208</td>\n",
       "      <td>15.594995</td>\n",
       "      <td>...</td>\n",
       "      <td>27.202839</td>\n",
       "      <td>10.503090</td>\n",
       "      <td>22.528412</td>\n",
       "      <td>4.584549</td>\n",
       "      <td>7.519589</td>\n",
       "      <td>3.517346</td>\n",
       "      <td>31.612198</td>\n",
       "      <td>33.847808</td>\n",
       "      <td>25691.160000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Time            V1            V2            V3            V4  \\\n",
       "count 284807.000000 284807.000000 284807.000000 284807.000000 284807.000000   \n",
       "mean   94813.859575      0.000000      0.000000     -0.000000      0.000000   \n",
       "std    47488.145955      1.958696      1.651309      1.516255      1.415869   \n",
       "min        0.000000    -56.407510    -72.715728    -48.325589     -5.683171   \n",
       "25%    54201.500000     -0.920373     -0.598550     -0.890365     -0.848640   \n",
       "50%    84692.000000      0.018109      0.065486      0.179846     -0.019847   \n",
       "75%   139320.500000      1.315642      0.803724      1.027196      0.743341   \n",
       "max   172792.000000      2.454930     22.057729      9.382558     16.875344   \n",
       "\n",
       "                 V5            V6            V7            V8            V9  \\\n",
       "count 284807.000000 284807.000000 284807.000000 284807.000000 284807.000000   \n",
       "mean       0.000000      0.000000     -0.000000      0.000000     -0.000000   \n",
       "std        1.380247      1.332271      1.237094      1.194353      1.098632   \n",
       "min     -113.743307    -26.160506    -43.557242    -73.216718    -13.434066   \n",
       "25%       -0.691597     -0.768296     -0.554076     -0.208630     -0.643098   \n",
       "50%       -0.054336     -0.274187      0.040103      0.022358     -0.051429   \n",
       "75%        0.611926      0.398565      0.570436      0.327346      0.597139   \n",
       "max       34.801666     73.301626    120.589494     20.007208     15.594995   \n",
       "\n",
       "       ...           V21           V22           V23           V24  \\\n",
       "count  ... 284807.000000 284807.000000 284807.000000 284807.000000   \n",
       "mean   ...      0.000000     -0.000000      0.000000      0.000000   \n",
       "std    ...      0.734524      0.725702      0.624460      0.605647   \n",
       "min    ...    -34.830382    -10.933144    -44.807735     -2.836627   \n",
       "25%    ...     -0.228395     -0.542350     -0.161846     -0.354586   \n",
       "50%    ...     -0.029450      0.006782     -0.011193      0.040976   \n",
       "75%    ...      0.186377      0.528554      0.147642      0.439527   \n",
       "max    ...     27.202839     10.503090     22.528412      4.584549   \n",
       "\n",
       "                V25           V26           V27           V28        Amount  \\\n",
       "count 284807.000000 284807.000000 284807.000000 284807.000000 284807.000000   \n",
       "mean       0.000000      0.000000     -0.000000     -0.000000     88.349619   \n",
       "std        0.521278      0.482227      0.403632      0.330083    250.120109   \n",
       "min      -10.295397     -2.604551    -22.565679    -15.430084      0.000000   \n",
       "25%       -0.317145     -0.326984     -0.070840     -0.052960      5.600000   \n",
       "50%        0.016594     -0.052139      0.001342      0.011244     22.000000   \n",
       "75%        0.350716      0.240952      0.091045      0.078280     77.165000   \n",
       "max        7.519589      3.517346     31.612198     33.847808  25691.160000   \n",
       "\n",
       "              Class  \n",
       "count 284807.000000  \n",
       "mean       0.001727  \n",
       "std        0.041527  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        0.000000  \n",
       "75%        0.000000  \n",
       "max        1.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print rangkuman statistik per feature\n",
    "pd.options.display.float_format = \"{:.6f}\".format\n",
    "data.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Frauds 99.83 % of the dataset\n",
      "Frauds 0.17 % of the dataset\n"
     ]
    }
   ],
   "source": [
    "#print persentase data transaksi yang termasuk non fraud dan fraud pada dataset\n",
    "print('No Frauds', round(data['Class'].value_counts()[0]/len(data) * 100,2), '% of the dataset')\n",
    "print('Frauds', round(data['Class'].value_counts()[1]/len(data) * 100,2), '% of the dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Class Distributions \\n (0: No Fraud || 1: Fraud)')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "colors = [\"#0101DF\", \"#DF0101\"]\n",
    "\n",
    "sns.countplot('Class', data=data, palette=colors)\n",
    "plt.title('Class Distributions \\n (0: No Fraud || 1: Fraud)', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class\n",
       "0    284315\n",
       "1       492\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mengelompokkan data berdasarkan Classnya\n",
    "data.groupby('Class').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##melakukan feature scaling\n",
    "##Feature Scaling adalah suatu cara untuk membuat numerical data \n",
    "##pada dataset memiliki rentang nilai (scale) yang sama. \n",
    "##Tidak ada lagi satu variabel data yang mendominasi variabel data lainnya.\n",
    "\n",
    "##membuat sub-sample dengan data 50/50\n",
    "##men-scale kolom Time dan Amount, karena kolom yang lain sudah di-scale\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "# RobustScaler menghilangkan median dan men-scale data berdasarkan quartile range-nya\n",
    "# StandardScaler menghilangkan mean dan men-scale data ke unit variance\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "rob_scaler = RobustScaler()\n",
    "\n",
    "data['scaled_amount'] = std_scaler.fit_transform(data['Amount'].values.reshape(-1,1))\n",
    "#data['scaled_time'] = rob_scaler.fit_transform(data['Time'].values.reshape(-1,1))\n",
    "\n",
    "data.drop(['Time','Amount'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scaled_amount</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.244964</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.342475</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.160686</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.140534</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.073403</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   scaled_amount        V1        V2       V3        V4        V5        V6  \\\n",
       "0       0.244964 -1.359807 -0.072781 2.536347  1.378155 -0.338321  0.462388   \n",
       "1      -0.342475  1.191857  0.266151 0.166480  0.448154  0.060018 -0.082361   \n",
       "2       1.160686 -1.358354 -1.340163 1.773209  0.379780 -0.503198  1.800499   \n",
       "3       0.140534 -0.966272 -0.185226 1.792993 -0.863291 -0.010309  1.247203   \n",
       "4      -0.073403 -1.158233  0.877737 1.548718  0.403034 -0.407193  0.095921   \n",
       "\n",
       "         V7        V8        V9  ...       V20       V21       V22       V23  \\\n",
       "0  0.239599  0.098698  0.363787  ...  0.251412 -0.018307  0.277838 -0.110474   \n",
       "1 -0.078803  0.085102 -0.255425  ... -0.069083 -0.225775 -0.638672  0.101288   \n",
       "2  0.791461  0.247676 -1.514654  ...  0.524980  0.247998  0.771679  0.909412   \n",
       "3  0.237609  0.377436 -1.387024  ... -0.208038 -0.108300  0.005274 -0.190321   \n",
       "4  0.592941 -0.270533  0.817739  ...  0.408542 -0.009431  0.798278 -0.137458   \n",
       "\n",
       "        V24       V25       V26       V27       V28  Class  \n",
       "0  0.066928  0.128539 -0.189115  0.133558 -0.021053      0  \n",
       "1 -0.339846  0.167170  0.125895 -0.008983  0.014724      0  \n",
       "2 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752      0  \n",
       "3 -1.175575  0.647376 -0.221929  0.062723  0.061458      0  \n",
       "4  0.141267 -0.206010  0.502292  0.219422  0.215153      0  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mengubah nama kolom Time Amount menjadi scaled_time dan scaled_amount\n",
    "scaled_amount = data['scaled_amount']\n",
    "#scaled_time = data['scaled_time']\n",
    "\n",
    "data.drop(['scaled_amount'], axis=1, inplace=True)\n",
    "data.insert(0, 'scaled_amount', scaled_amount)\n",
    "#data.insert(1, 'scaled_time', scaled_time)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rincian class pada data training Counter({0: 227453, 1: 392}) dan testing Counter({0: 56862, 1: 100}) \n"
     ]
    }
   ],
   "source": [
    "#mengambil seluruh data fraud dan non-fraud\n",
    "fraud_data_all = data2.loc[data2['Class'] == 1]\n",
    "nonFraud_data_all = data2.loc[data2['Class']==0]\n",
    "\n",
    "#menggabungkan data fraud dan non-fraud\n",
    "normal_distributed_data_all = pd.concat([fraud_data_all, nonFraud_data_all])\n",
    "normal_distributed_data_all.shape\n",
    "#normal_distributed_data_all.head()\n",
    "# data2.head()\n",
    "#fraud_data_all.shape\n",
    "\n",
    "X2 = normal_distributed_data_all.drop('Class', axis=1)\n",
    "y2 = normal_distributed_data_all['Class']\n",
    "#X2 --> variabel label untuk prediksi\n",
    "#y2 --> variabel feature, semua kolom kecuali X (Class)\n",
    "\n",
    "#Memecah dataframe baru menjadi training dan test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size=0.2, random_state=0)\n",
    "\n",
    "#mengubah ke array agar lebih mudah untuk algoritma klasifikasi\n",
    "X_train2 = X_train2.values\n",
    "X_test2 = X_test2.values\n",
    "y_train2 = y_train2.values\n",
    "y_test2 = y_test2.values\n",
    "\n",
    "\n",
    "print (\"Rincian class pada data training {} dan testing {} \" .format(Counter(y_train2), Counter(y_test2)))\n",
    "#y_train2.shape\n",
    "#y_test2.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(227845, 29)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56962, 29)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_resampling(X, y, sampling, ax):\n",
    "    X_res, y_res = sampling.fit_resample(X, y)\n",
    "    ax.scatter(X_res[:, 0], X_res[:, 1], c=y_res, alpha=0.8, edgecolor='k')\n",
    "    # make nice plotting\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.get_xaxis().tick_bottom()\n",
    "    ax.get_yaxis().tick_left()\n",
    "    ax.spines['left'].set_position(('outward', 10))\n",
    "    ax.spines['bottom'].set_position(('outward', 10))\n",
    "    return Counter(y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_function(X, y, clf, ax):\n",
    "    plot_step = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                         np.arange(y_min, y_max, plot_step))\n",
    "\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, Z, alpha=0.4)\n",
    "    ax.scatter(X[:, 0], X[:, 1], alpha=0.8, c=y, edgecolor='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time untuk NearMiss: 11.986906 detik\n",
      "Class pada dataset Counter({0: 39200, 1: 392}) \n"
     ]
    }
   ],
   "source": [
    "#implementasi teknik ENN untuk undersampling\n",
    "t0 = time.time()\n",
    "\n",
    "##renn = RepeatedEditedNearestNeighbours (sampling_strategy='not minority', n_neighbors=5)\n",
    "nm = NearMiss(sampling_strategy=0.01, version=1)\n",
    "##from imblearn.under_sampling import \n",
    "cc = ClusterCentroids ()\n",
    "enn = EditedNearestNeighbours(sampling_strategy={0 : 10000, 1 : 396}, n_neighbors=3)\n",
    "##X_renn, y_renn = renn.fit_sample(X_train2,y_train2)\n",
    "X_nm, y_nm = nm.fit_resample(X_train2, y_train2)\n",
    "#X_enn, y_enn = enn.fit_resample(X_train2, y_train2)\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "print(\"Training time untuk {}: {:3f} detik\" .format(nm.__class__.__name__, t1-t0))\n",
    "print (\"Class pada dataset {} \" .format(Counter(y_nm)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39592, 29)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_nm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.manifold import TSNE\n",
    "# from sklearn.decomposition import PCA, TruncatedSVD\n",
    "# import time\n",
    "# import matplotlib.patches as mpatches\n",
    "\n",
    "# def plot_dim (x,y):\n",
    "#     #implementasi T-SNE\n",
    "#     X_reduced_tsne = TSNE (n_components=2, random_state=10).fit_transform(x)\n",
    "    \n",
    "#     #impelemntasi PCA\n",
    "#     X_reduced_pca = PCA (n_components=2, random_state=10).fit_transform(x)\n",
    "    \n",
    "#     #implementasi Truncatedd SVD\n",
    "#     X_reduced_svd = TruncatedSVD (n_components=2, algorithm='randomized', random_state=2).fit_transform(x)\n",
    "    \n",
    "#     #membuat gambar\n",
    "#     f, (ax1,ax2,ax3) = plt.subplots(1,3, figsize=(24,6))\n",
    "#     f.suptitle ('Clusters using Dimensionality Reduction', fontsize=14)\n",
    "    \n",
    "#     blue_patch = mpatches.Patch (color='#0A0AFF', label ='No Fraud')\n",
    "#     red_patch = mpatches.Patch (color ='#AF0000', label ='Fraud')\n",
    "    \n",
    "#     #scatter plot t-SNE\n",
    "#     ax1.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], s=4, c=(y == 0),\n",
    "#                 cmap='coolwarm', label='No Fraud', linewidths=2)\n",
    "#     ax1.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], s=4, c=(y == 1),\n",
    "#                 cmap='coolwarm', label='Fraud', linewidths=2)\n",
    "#     ax1.set_title('t-SNE', fontsize=14)\n",
    "\n",
    "#     ax1.grid(True)\n",
    "\n",
    "#     ax1.legend(handles=[blue_patch, red_patch])\n",
    "\n",
    "\n",
    "#     # scatter plot PCA\n",
    "#     ax2.scatter(X_reduced_pca[:,0], X_reduced_pca[:,1], s=4, c=(y == 0),\n",
    "#                 cmap='coolwarm', label='No Fraud', linewidths=2)\n",
    "#     ax2.scatter(X_reduced_pca[:,0], X_reduced_pca[:,1], s=4, c=(y == 1),\n",
    "#                 cmap='coolwarm', label='Fraud', linewidths=2)\n",
    "#     ax2.set_title('PCA', fontsize=14)\n",
    "\n",
    "#     ax2.grid(True)\n",
    "\n",
    "#     ax2.legend(handles=[blue_patch, red_patch])\n",
    "\n",
    "#     # scatter plot Truncated SVD\n",
    "#     ax3.scatter(X_reduced_svd[:,0], X_reduced_svd[:,1], s=4, c=(y == 0),\n",
    "#                 cmap='coolwarm', label='No Fraud', linewidths=2)\n",
    "#     ax3.scatter(X_reduced_svd[:,0], X_reduced_svd[:,1], s=4, c=(y == 1),\n",
    "#                 cmap='coolwarm', label='Fraud', linewidths=2)\n",
    "#     ax3.set_title('Truncated SVD', fontsize=14)\n",
    "\n",
    "#     ax3.grid(True)\n",
    "\n",
    "#     ax3.legend(handles=[blue_patch, red_patch])\n",
    "\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.utils import resample\n",
    "\n",
    "# #x_renn_plot, y_renn_plot = resample(X_renn,y_renn, n_samples = 50000, random_state = 0)\n",
    "# x_nm_plot, y_nm_plot = resample(X_nm,y_nm, random_state = 0)\n",
    "\n",
    "# plot_dim (x_nm_plot,y_nm_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\muhsin ahadi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\muhsin ahadi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\muhsin ahadi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\muhsin ahadi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\muhsin ahadi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\muhsin ahadi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training logistic regression untuk under sampling NearMiss memakan waktu: 9.261172 detik\n"
     ]
    }
   ],
   "source": [
    "classifiers = {\"LogReg\" : LogisticRegression()}\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "t0 = time.time()\n",
    "for key, classifier in classifiers.items():\n",
    "    classifier.fit(X_nm, y_nm)\n",
    "    training_score = cross_val_score(classifier, X_nm, y_nm,\n",
    "                                    cv=5)\n",
    "#     print(\"Classifiers: \", classifier.__class__.__name__, \"training skor akurasinya\", round(training_score.mean(),2)*100,\n",
    " #        \"%\")\n",
    "t1 = time.time()\n",
    "\n",
    "print ('Training logistic regression untuk under sampling {} memakan waktu: {:3f} detik' .format(nm.__class__.__name__, t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\muhsin ahadi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "c:\\users\\muhsin ahadi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\muhsin ahadi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\muhsin ahadi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\muhsin ahadi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\muhsin ahadi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\muhsin ahadi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\muhsin ahadi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\muhsin ahadi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\muhsin ahadi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\muhsin ahadi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\muhsin ahadi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\muhsin ahadi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\muhsin ahadi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\muhsin ahadi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\muhsin ahadi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\muhsin ahadi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\muhsin ahadi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\muhsin ahadi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\muhsin ahadi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\muhsin ahadi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\muhsin ahadi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\muhsin ahadi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\muhsin ahadi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\muhsin ahadi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\muhsin ahadi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\muhsin ahadi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\muhsin ahadi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\muhsin ahadi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\muhsin ahadi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\muhsin ahadi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\muhsin ahadi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\muhsin ahadi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\muhsin ahadi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\muhsin ahadi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\muhsin ahadi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\muhsin ahadi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\muhsin ahadi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=0.001, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='saga', tol=0.0001, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# t0 = time.time()\n",
    "# Logistic Regression \n",
    "log_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], \n",
    "                  'solver' : ['liblinear','saga'], 'max_iter' : [100], 'multi_class' : ['ovr'],\n",
    "                 'class_weight' : ['balanced']}\n",
    "\n",
    "\n",
    "\n",
    "grid_log_reg = GridSearchCV(LogisticRegression(), log_reg_params)\n",
    "grid_log_reg.fit(X_nm, y_nm)\n",
    "# We automatically get the logistic regression with the best parameters.\n",
    "log_reg = grid_log_reg.best_estimator_\n",
    "\n",
    "print (log_reg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# Create a DataFrame with all the scores and the classifiers names.\n",
    "#t0 = time.time()\n",
    "log_reg_pred = cross_val_predict(log_reg, X_nm, y_nm, cv=5,\n",
    "                             method=\"decision_function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report Logistic Regression:\n",
      "Recall Score: 0.90\n",
      "Precision Score: 0.50\n",
      "F1 Score: 0.64\n",
      "Accuracy Score: 0.99\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score\n",
    "\n",
    "# t0= time.time()\n",
    "y_pred = log_reg.predict(X_nm)\n",
    "\n",
    "print ('Classification Report Logistic Regression:')\n",
    "print('Recall Score: {:.2f}'.format(recall_score(y_nm, y_pred)))\n",
    "print('Precision Score: {:.2f}'.format(precision_score(y_nm, y_pred)))\n",
    "print('F1 Score: {:.2f}'.format(f1_score(y_nm, y_pred)))\n",
    "print('Accuracy Score: {:.2f}'.format(accuracy_score(y_nm, y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\muhsin ahadi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\muhsin ahadi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 2 features per sample; expecting 29",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-54fb2c9dda06>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#log_reg = make_pipeline(nm, LogisticRegression())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_nm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_nm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mplot_decision_function\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX_nm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_nm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0max1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Decision function for {}'\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-4505e79d6cd0>\u001b[0m in \u001b[0;36mplot_decision_function\u001b[1;34m(X, y, clf, ax)\u001b[0m\n\u001b[0;32m      6\u001b[0m                          np.arange(y_min, y_max, plot_step))\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mxx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mZ\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontourf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mZ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\muhsin ahadi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m         \u001b[1;31m# update the docstring of the returned function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\muhsin ahadi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\imblearn\\pipeline.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X, **predict_params)\u001b[0m\n\u001b[0;32m    348\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m                 \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 350\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpredict_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mif_delegate_has_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'_final_estimator'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\muhsin ahadi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    279\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m         \"\"\"\n\u001b[1;32m--> 281\u001b[1;33m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    282\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    283\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\muhsin ahadi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m             raise ValueError(\"X has %d features per sample; expecting %d\"\n\u001b[1;32m--> 262\u001b[1;33m                              % (X.shape[1], n_features))\n\u001b[0m\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m         scores = safe_sparse_dot(X, self.coef_.T,\n",
      "\u001b[1;31mValueError\u001b[0m: X has 2 features per sample; expecting 29"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1,ax2) = plt.subplots(1,2, figsize = (20,6))\n",
    "clf = LogisticRegression().fit(X2,y2)\n",
    "clf  = make_pipeline(nm, LogisticRegression())\n",
    "#log_reg = make_pipeline(nm, LogisticRegression())\n",
    "clf.fit(X2, y_nm)\n",
    "plot_decision_function (X_nm, y_nm, clf, ax1)\n",
    "ax1.set_title('Decision function for {}' .format(nm.__class__.__name__))\n",
    "\n",
    "plot_resampling (X2, y2, nm, ax2)\n",
    "ax2.set_title('Resampling using {}' .format(nm.__class__.__name__))\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWgAAAEmCAYAAABPtwrJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3XmcFMX5x/HPF1BEEVBRo6BCFOPNLXifAbziEY0kGvEK6i/G23hGUeMZDcYzwQuI9xnxCgIG1MghIJeigoIBQRRBREB0d5/fH1UDvcPO7Czs7szsPm9e/WKmurq7unv2mZrq6mqZGc455wpPg3wXwDnnXMU8QDvnXIHyAO2ccwXKA7RzzhUoD9DOOVegPEA751yB8gC9DiT1k7RAkkk6tRrW1yauq0s1FK9gSTow7mfLfJclG0mnSvpuHdfRT9K06ipTXRQ/C8fnuxyFqM4FaElbSvqbpE8krZT0uaTXJB1ezdvZDbgWOBvYCniqGlY7J65rUjWsK6NEgFwiacO0eTvHeVUKoJIGSno5x+zvEPbz6yoUO317tRH4ngJ+mmN5Mn253g4csLYFiF8SlpgWSHpJ0q5ru84CtBXwUr4LUYjqVICW1AaYCPQErgD2AA4FXgH+Xs2b2yH+/y8z+8LMVqzrCs2sNK6rZF3XlaMlwAlpaWcA/6upDUpaz8x+iPtZ0HdJmdkKM/tyHdfxnZmt9RdRtJwQxLYGjgA2Al6RtP46rrdSktar6W3Ez8LKmt5OUTKzOjMBrwLzgKYVzNsk8Xpb4AVgaZyeB1on5vcDpgG9gU9inn8BLRPzLTnF9IHAy2nb7QdMS7zfHRgBfBvXOxk4KM5rE9fXJZF/f2As8D2wAOgPrJ+YPxK4D7gJWAh8Sai1NchynA6M27keGJVIXy9u47o4P7W/DYGHgFnACmAG8MfUNio6HnEbqf35NfBGXPbcxPZT638IeB9oktje2+nHMttxreh8A4OAxXG7w4Fd0/KcTvgyWk6owf1f6lzG+acC3yXebwO8CCyKy3wI9I7z0vd/ZKZyAn2AqcDKeLwHZtmPcmWIaUfFbeyeSFM8J5/E/Z0KnJy2XDdCBeZ74D3g8NS5SvtcHA6MA34Ajkxsc0JcdhZwI+U/h8cBU+K2FwGjgC0rO26JY3d82t/I8MS6BgLNE/MHAi8D5wOfx3P8CLBhvmNQdU95L0C17QhsCpQBV1aST/FD+g7QFegCjAHGA4p5+gHfEYL4HsBewGfAP+L8psCZ8YP1E+AnyQ9O2vbK/YHGP5xHgZ0ItfBjgb3ivDYkAjTQClhGqP3vDBwJfAHckVjfSEJN+HpgR+BXQAnw6yzHIPWHuGP8I9g+ph9L+AM/iPIBdL24/q6xjL8CvgHOSByPp4BhqeMBrJ/Yn9nA8UBboDVrBuiNgI+Be+P7a+J+bpFlH8od1wrmv0gIBPsT/uCHEJqQUl8Ce8XPy2XxOPwO+IrsAfqluI/t4770AnrFeV3jPvWM+79phvN/FiHIXQT8DOgMXJplP9LL0AJ4Im5rp0T6jcBHsUxtgd/Ez84RiXP0FfA4sCvwc8KXYkUBeirQg9C8s3ncp2+B04Dt4+fjI+D2uNxPCMH84njOdyP8fWxZ2XGL81cFaGBDQtD9VzxvB8TPxnOJ/AMJn/kHCH8XPQifxyvyHYeqPa7luwDVtiOwZzzRx1aS7+dAKdAmkfbT+Md6aHzfL/4RJb+1rwJmJt4fn/xjTnxwKgvQ3wJ9MpStDeUD9I3ATBK14fgHu5JYWyAE6NFp6xkGPJjlGKT+EFsSAuuNMf1l4GrSAmiGddwCDK9k31P7c3Gm7SfSusQ/8uuBH4HDKjmP5Y5r2rx2cf37J9Kaxz/qM+P7J4B/py03gOwBegpwbS7nLsv5nwvcUoXP9alxvd8RAm6qhv5iIs9GhC/a/dKWvRN4Nb4+i1AbbZKY/xsqDtC/TFvPm8Cf0tKOiWUS0Ckut12Gfch43OL8ZID+XTxPG1fwedkh8VmbAzRK5Hkg+XmsK1NdaoNWjvl2BuaZ2exUgpl9Smga2SWR7zMzW5J4Pw/YYl0LCfwVeFDSG5KukrRTJWUdbWZlibS3CbXTHRJpU9KWq0pZHwL6SNqG8OU1sKJMks6WNF7SV7Fnw4WEpqJcjK8sg5mNJ3wh/QkYYGav5bjuiuxM+MIdnVj/EkLNMHWOdyL8jE8aW8l6/wZcLWm0pD9L6lyVQknagvCraERVliM0C3Qg1LbPIjQxnZWYvwuwAfBvSd+lJuAcQo0Xwv5Os/LXSjLtb/r56gxclbbuxwlfDD8hNNMNB6ZJek7SOZI2TyxfleO2MzDFzJYm0t4hnM/k3+cHVv5aTXX9fRaUuhSgZxC+ZXeuJJ9ivook03+sYF5lx6uMNb8oyl1kMbN+hA/av4C9gSmSTs9DWVOGE35RDAbeMLO5axRCOpFQGxtI+LnbgdDunetFqmWVZZAkYN9Ylu3j+7WVbVlL5Ml0bCte0Owhwk/0RwjNIu9I6ldN5apk0zbTzD40swHAY4RfACmpc30U4dykpl0JP/9T2851f9PPVwPCdYnkuvcg/FL5ysxK43Z6ECoLZwAzJLWPha/KcauNz3zRqDM7ZGaLgKHAuZKaps+X1CK+/ABoFXt8pOb9lHCF/IN1LMZXhKvtSR0qKOsMM7vLzI4g1GDPzLC+D4C9JCXP076EpoBP1rGsqbKUEQLvgbEsFdkXGGtm95jZRDObyeqaWcoPhIt7a+siwk/l/YHuwB/WYV0fED7be6USJDUjtGmmzvF0QrNYUvr7NZjZXDMbYGa/IrSV942zfoj/ZzwGZraA0L56SA77kE1/oJOk4+L7DwjNXtvFQJ6cPot5pgO7S2qSWE+l+xtNJLR3p697ZqoWa8FoM7uO0B4/DzgxtYIsxy3dB0B7SRsn0vYmnM/pOZa3zqgzATr6P8I38HhJJ0j6maSdJJ3D6maA4YSfZI9J6hz7rT5G+BC+sY7bfwPoKOl0STtI+iOwT2qmpCaS7o39kNtI6kYIfpm+GO4jfHHcF/snH0Fo+73HzJavY1mT/ky4GPR8hvkfEwLCYZLaSfoTa/btnQ3sFo95y6p0z4o1rRuBvmb2DuGn+a2xr3k2G0jqkDbtaGYzCBcJ/yFpP0m7Ey7Mfkv4aQ5wF9BD0qVxn84gXCTNVs6/Seol6aeSOhAudqXO3ZeEduCesS9+8wyruRG4QNKFknaMZb64kv0sx8y+BR4ErpPUIDYH3A7cnvjsdYjNUqlA+Bjh18kDknaRdChwZWqVlWzyeuA3kq6XtFv8mzpe0m3xuHSXdLWkrpK2BX5B6LnxQQ7HLd1jhBr8YEm7S9of+AfwfKwY1C/5bgSv7olQg70b+JRQq5gHvEbiohOh7fRfrO5m9wIVdLNLW++plL9gtMZFwsSy8wkXOlLd36bFeesTAsRnibINAJrF+W3I3M0u1SWrP9A4MX8kIWAnyzCQ7F3UDiTLRcD0+bHcDxG6M30TX18DzE4ssznwejyeRvludukXzlatn9B2Og14OC3PPwlfpI0zlLEfa3ZtM2B8nJ9rN7s5cf5LhF4IK7Kc87sJTWnfE34tPQm0Ssw/k9Btr5Ts3ezOIASoHwi9VR6uaB8rKkPaZ/hH4DfxvQi/OlK16a8IF4t/nlimO6F73cr4/y/jMetW2eeC0HzxFqE9/FtCO/W5cd7OhL+xBXHdM4E/VuG4rbpIGN+nuqKuiOdvIBV0s6vg85CxV0+xTqluZc7Ve5L6E3ry7J7vstQGSUcTKidbmNnCfJfHralRvgvgXL5IupRQy/yOcMfp2az+2V/nSOpD+GU5h9BX+U7gJQ/OhcsDtKvPugCXEPpIzyIMD/C3vJaoZm1J6I2xFaFp5RXCjTquQHkTh3POFai61ovDOefqDA/QdZSkLnF4yjb5LkttU9o4zunvXXnyMasLlgfoPFGWQeslzZZ0ST7KlQtJDSVdJmm6pOWSFsfbwM/Ld9kyyHlc53Wh1WNCf53eD1rSSEn31HQZ4rYGxnI8WMG82+K85Njd6zRmtas5HqBdRso83vC1wKWEC067Ef647yZcbCs4Vg3jOlfRhsDltbg9ACQ1StwiPwc4UdJGyfnAb0kb79uqZ8xqVwM8QBe4RK3sl5KGxRrrB5J+npavl6QPJX0v6S3CmAfp69pb0qi4js8l3a9wC3Rq/siYdrukr4D/ZijWL4C/m9mTZvapmU0xs0FmdkNiXV0lvS5poaRvJb0taa/kSuJ+nSPpxVimjyUdJKm1pKGSlkmaJKlTYplTFQbsOSrm/17SfxRu1890DNObPPpJmiapt8KTd5ZK+lfy10wMdv3jr4PF8fX9kkZm2k7CXcD5klplKZMk/TFuf4WkqZJOTstzi6SP4vzZsfa7QQX7caqkTwg3iaQC8hTCzSG/SqzyCMLNIuX2Ib2JQ+EOvhHxvC2VNFnSQXHeepLukjRP4YlFcyTdksMxcWvBA3TxuJHwh98eeBd4UnHMEYWR6P5F6NPbgVCbvS25sMLtzq8TxkVuTxhgvQPwcNp2TibclbYfcEqGsnwBHChpyyzl3ZhwN+B+hDEfJgGvas0mnasJd5a1J9yd9gThTsX7gI6Euy0Hpi3TmFCLP40w3kZD4IVE7TEXbQhjRRxLuEuuI+EYp1xCuIvvTMIdeA0Iw3Pm4hnCyHnXZ8nzZ8Idhb8nDJ51M+HW9CMSeZYR7nbcmTCMQW/CsLdJqbGfTyAcw+8T8x6Ky6ecThiwqLKuW48T7obdk3Bc+iXWex7hmPUmDJZ0ImFsaFcT8n0rY32dyH5b7Wzgkvi6Tcx3VmJ+q5i2b3x/E2G8DCXyXB3ztInvBwMPpW2nQ8yzRXw/kjDUY2Vl34VwS3EZYdD3BwkBX1mWEeGP/uREmgE3J97vFtMuynScWD0+8j6JPNsRbq8+NJEneYt2+vt+VD7e93zg8rTyf0i8hTvDPqbOVRdCs08J8fZyErfkk8P4zRnWf3ZaGfsRbvfeMi3fQMLY3pvE7bQjDAu6knCL+EASt0pTtTHL7yLchp3xXPtUfZPXoItHcsznefH/1Pi3OwNjLP4FRaMprzNwssqP6ZtqwkiOTDehsoKY2QeEYNqNEJw3A54mPCevAYSxjyX9IzZDLCGM0bEFa44hndyvBfH/qRWkJcf6LSMxlrOFEdvSx/OuTMbxvuMFvp+kbcMIv1xyYmajCKMr3lzB7FzGb0ZhQKK3JX0R5/dnzeM318IoeRWVYTHhVu7TCY/ZGmlmuTxvMtuY5QMJX+wfKwz8dYTKj7boqpEf2Pz5Nv5f0YW1FoTBlpJWjX+bCMSp85fLT/sGhGCaHNO3PaF2lXyKeKVjN8cylJnZu2bW38yOJdRSDyMM7gRhoKKuhIH9947bm8uaY0gnx/W1LGnV/VnNZTzhdb2L6zLgCEn7paVXOn6zpO6Epp+hMV9Hwq+i9FECKztfDxOaqk5nzeasClmWMcvNbCLhl8KVcT8GAcM8SNcMv9U7f2YQaoKdSYztHC92Nadq7XofAL+UpETw7p6WZyLh53ZNDdmYGj4yNRb3vsB5ZvYKQGyvTh8re201IAT/d+K6tyUMy1ot4wWb2RJJXxDaYP8Tt6G4zS+qsJ5pkgYTrgckn1qdHL850xC3+wCfW/kLr9tVaUeCEYRR81oSAm5OLAzZOgO4S9L9hLb4h+O8pYR29mckDSQ803MHQjObq0YeoPPEzJYq9FP9i6SVhJ/62wC3EoYXfasKq/s7YajMOyXdRxiu8ey0PLcCYyT9nTC+7lLCY5COMrOzqAJJzxKaR94hBKy2hJ/yX8Y0CH+sJ0saS2hzvY3Vg9qvqxLCvp5PaGPtT2gLH15N64cwJscfJX1MCKhnEb5g5ldxPdewOnBNg1XnPjV+swjP/GtK+FIts/DUlI8JD5Y4idBc1ZPwdPQqMTOTtAehzXhlZfkVBvS/nRCAZxPG79iX+HgsSRcRjsEk4nCnhF+DazyJx607/1mSX+cTaiW3EALMIEL765Fp7clZxXbF4wgDoU8mNCtcnpZnCqH5oQ0wKua7mdVtvFUxlNBlawghkPyTMMb1wRaebAPhJ3VTQpv2k4T9nL0W26rISkKPi8GEwNEAOK4qxywHtxP26xFCDRFCe+73GZeogJnNIVxY2yBt1p8IF+cuIZz7YYTxmWfF5V4C/kK4cDiF8LzIa6q+G+ELwcIg/7koZfVY2h8R9nk04Yk3EL7YLyW0z08kNM0cZtX7AAkX+WBJrqhIOpXQG2KNx5rVwrYnAv81s3V5HJdzOfMmDucqENt7exJ+bTQiPEOvPZmfpedctfMA7VzFygi9H/5CaEL5gPBTfnxeS+XqFW/icM65AuUXCZ1zrkB5gHbOuQLlbdA5atlyY2vTZvN8F8PlaMKEXO5odoWjDLOyqgx2BUDPXnvY1wtzexbDhAmzhppZryoXLY88QOeoTZvNGfvuDZVndAWhcaNCfXaAq0hpWfrIBrlZuHApY8Zdl1Pe9Rv2WePhGIXOA7RzrogZZiX5LkSN8QDtnCtaZkZpWaV3sBctD9DOuSLmNWjnnCtQHqCdc64wmWFlHqCdc64weQ3aOecKkTdxOOdcQTIrw0pX5LsYNcYDtHOuiBl4G7RzzhUg8wDtnHOFy9ugnXOu8AhDXoN2zrkCZGVQUqXn+BYVD9DOuSJmqA43cfiA/c654mVAWWluUw4kzZY0VdIkSeNj2qaShkmaEf/fJKZL0l2SZkqaIqlTYj19Yv4Zkvok0jvH9c+My2YdA9sDtHOuiIU26FymKjjIzDqYWZf4/nJghJm1A0bE9wCHAe3i1Be4H0JAB64FugF7AtemgnrM0zexXNYHCHiAds4VMavWGnQGRwOD4utBwDGJ9MEWjAFaSNoK6AkMM7NFZrYYGAb0ivOamdloC0/rHpxYV4W8Ddo5V7yq1g+6ZarZIhpgZgPS1wi8LsmAf8T5W5rZ/LA5my9pi5i3FTAnsezcmJYtfW4F6Rl5gHbOFS8zVPJDrrkXJpotMtnHzObFIDxM0odZ8lbUfmxrkZ6RN3E454qaykpzmnJhZvPi/18CLxDakBfE5gni/1/G7HOBbRKLtwbmVZLeuoL0jDxAO+eKWPW1QUvaSNLGqddAD2AaMARI9cToA7wYXw8BTom9OboDS2JTyFCgh6RN4sXBHsDQOG+ppO6x98YpiXVVyJs4nHNFS2Y5145zsCXwQuz51gh43Mz+Leld4GlJZwD/A06I+V8FDgdmAsuB0wDMbJGkG4B3Y77rzWxRfH0OMBBoArwWp4w8QDvnils1BWgz+xRoX0H618AhFaQb8PsM63oYeLiC9PHAbrmWyQO0c654Ve0iYdHxAO2cK2KGysryXYga4wHaOVe8Urd611EeoJ1zRcw8QDvnXKGSeROHc84VHvMatHPOFSYzKKm740F7gHbOFbVqvFGl4HiAds4VMQPvZueccwXI8ADtnHOFyWvQzjlXkGSGSn7MdzFqjAdo51xx8xq0c84VIG+Dds65QuVt0M45V5gMKMv6WL+i5gHaOVfE/E5C55wrTF6Dds65Auaj2TnnXCEyr0E751xBquNNHA3yXQBXPbZvewEd9riczh2vpFvXP5Wbd8ftr9CowcksXLgUgMWLl/HL4/rTsf0VdO92DdOmzVmV987+r7HHbpfRfvfLOek39/D99+GBnG+88T5dO19F+90v57RT/05JSd0dQaw2PfDgXcyb/yGTJr+9Kq19+93473+HMn7CSMaMHUHXrp0AuPjicxk/YSTjJ4xk0uS3WfnDl2yySQsaN27M6NHDmDBxFJOn/Jdrr70sX7uTH2WW21SEPEDXIcPfuIoJ793E2HdvWJU2Z87XDB8+jW233WxV2s03vUj79tvx3uSbGTjobC684J8AfP75Iu65+3XGvnsDk6feQmlpGU89OYaysjJOP/UfPPbEuUyeegvbbtuSwYPeqvX9q4sGD3qCIw7/Vbm0W27txw033EaXzgdyXb+bueWWawG444576NL5QLp0PpCrr7qBN0e9w+LF37By5UoOPfQYOnc6gM6dDqBnz0Po1q1LPnan1pmBlVhOUzHyAF3HXXzRo9xya28krUqbPv1zDj5kVwB22mlrPpu9kAULlgBQUlLKihU/UFJSyvLlP7DV1pvw9dff0bhxI3bccSsADv35bjz//Lu1vzN10FtvjWbRosXl0syMjZttDECz5s2YN/+LNZY7sfdxPPnUc6veL1u2DID11luPRus1wqw4A1KVGVCW41SEPEDXEZI4rOct7Nnlah4Y8AYALw2ZQKutN6F9++3K5d1jj215IQbYceM+4bPPFjJ37iJatdqUiy4+nLbbnU/rrc+lefMN6dFjd1q23Jgffyxl/PhPAXj+2XHMnfN17e5gPXLRhVdx663XMWv2FG677XquuvKGcvObNGlCz56H8PxzL61Ka9CgAeMnjGT+Fx8yYvgoxo2bUNvFzh8P0FUnySTdkXh/iaR+VVj+VElfSZoUp8E1VM5+ki6piXXXpjffvoZ3J9zIy69eyv33DefNNz/kppuG0O/649fIe9nlR/HNN8vo3PFK7r3ndTp23I5GjRqwePEyhgyZyMxP+zPn87tZtmwljz36NpJ47IlzufiiR+ne7RqabtyERo0a5mEv64ezzj6Niy++mrZt9uDii6/igQfuKjf/yKN68s47Y1m8+JtVaWVlZXTpfCDbbbs7Xbt2ZNddd6rtYueP5TgVoZqsQa8EjpPUch3W8ZSZdYjTKekzJXkvlGjrrTcBYIstmnP0MZ15c9R0Zs/6ik4drmT7thcwd+4iuna+mi+++IZmzTbkoYfPYsJ7NzFw0Nl89dVS2rbdnBHDp9G2zeZsvnkz1luvEcce24XR78wAYK+92jHqzWsYM/Z69tt/J3Zot2U+d7dOO+WU3rzwfKgdP/vMi3Tds1O5+SeeeBxPPvl8hcsuWfIto0b9l549D6nxchYEAytTTlMxqskAXQIMAC5MnyFpO0kjJE2J/2+b60oljZR0k6RRwPmSjpI0VtJ7koZL2jLmK1czljRNUpv4+ipJH0kaDvxs3XYz/5Yt+56lS1esej1s2DS6dP0p8xfcxyez7uSTWXfSuvWmvDvhz/zkJy345ptl/PBDuD32oQdHst/+O9Gs2YZss+1mjB07k+XLV2JmvPHG++y0cysAvvwytFGvXPkjf7ntJfqeVU8CQB7Mm/cFBxywDwAHH7w/M2Z8smpes2Ybs//+ezPkxddWpbVsuRnNmzcDYIMNNuCQQw7go49m1G6h86lEuU1FqKZroPcCUyTdlpZ+DzDYzAZJOh24CzimguVPlLRvfP03M3skvm5hZgcASNoE6G5mJulM4I/AxZkKJKkz0BvoSNj/iUBRN9gtWPAtxx93JxAu8vX+9d706tU+Y/7p0+dxWp+/07BhA3bepRUPPPg7ALp124HjfrknXTtfTaNGDenQcTt+1/cgAG7/yyu8+sokysrKOOvsQzn44F1rfsfqgUcfG8ABB+xDy5abMfuzqVx33S2cfdYF/LX/TTRq1IiV36/knLMvWpX/mGOPZNiw/7B8+fJVaVtttSUPP3IvDRs2pEGDBjz7zL945ZXX87E7tS/WoOsq1dTVXknfmVlTSdcDPwIrgKZm1k/SQmArM/tR0nrAfDNrmbb8qUAXMzs3LX0kcK2ZjYrvdwfuALYC1gdmmVmv2N79nZndHvNNA44kfBFsambXxPS/AvNS+dK21RfoC7Dttpt1/nT236rj0Lha0LjRefkugquC0rIlmFW9mtt5m4Y2+oINcsrb+JLlE8ysqPof1kYvjjuBM4CNsuSp6rfEssTru4F7zGx34CwgdbZKKL9/ybOY0/bMbICZdTGzLptv3qyKRXTO1QpTblMOJDWMzaUvx/dtYxPqDElPSVo/pjeO72fG+W0S67gipn8kqWcivVdMmynp8lzKU+MB2swWAU8TgnTKO4RmBoCTgLfTl6uC5sDn8XWfRPpsoBOApE5A25j+JnCspCaSNgaOWodtO+fyqfovEp4PTE+8vxXob2btgMWsjmNnAIvNbAegf8yHpF0IsW1XoBdwXwz6DQlNvocBuwC/jnmzqq1+0HcAySaM84DTJE0Bfks4KGurH/CMpLeAhYn054BNJU0CzgE+BjCzicBTwKSYx2+Jc66YlTXIbaqEpNbAEcCD8b2Ag4FnY5ZBrL5WdnR8T5x/SMx/NPCkma00s1nATGDPOM00s0/N7AfgyZg3qxq7SGhmTROvFwAbJt7PJux4tuUHAgMrSD8w7f2LwIsV5FsB9Miw7huBG7Nt3zlXBExYabXVM+8kdDLYOL7fDPjGzFJPBJgLtIqvWwFzAMysRNKSmL8VMCaxzuQyc9LSu1VWIL+T0DlX3HKvQbeUND4x9U2tQtKRwJdmluzRVVG7iFUyr6rpWfmNHs65omVV62a3MEsvjn2AX0g6nNChoBmhRt1CUqNYi24NzIv55wLbAHPjDXPNgUWJ9JTkMpnSM/IatHOuiAnKcpyyMLMrzKy1mbUhXOR7w8xOAv4DpMZL6MPq5tQhrO6UcHzMbzG9d+zl0RZoB4wD3gXaxV4h68dtDKls77wG7ZwrapZjF7q1dBnwpKQ/A+8BD8X0h4B/SppJqDn3DmWx9yU9DXxA6Or7ezMrBZB0LjAUaAg8bGbvV7ZxD9DOueJlYCXVO3CXmY0ERsbXnxJ6YKTn+R44IcPyFXZCMLNXgVerUhYP0M65IqaarkHnlQdo51zxMnLq41ysPEA754paXR4syQO0c65oGTV+kTCvPEA754qXyZs4nHOuUJVV363eBccDtHOueHkN2jnnCpdfJHTOuQLkFwmdc65Q1fFnEnqAds4VMWGl1XurdyHxAO2cK2rexOGcc4XImzicc64wGcLMu9k551xB8hq0c84VIoMyv1HFOecKk9egnXOuAHkbtHPOFSrvxeGcc4WrXvaDltQs24Jm9m31F8c556qmXgZo4H3CWCTJvU+9N2DbGiyXc85Vykz1czxoM9umNgvinHNroy5fJMxpzyT1lnRlfN1aUueaLZZzzuWmzJTTVIwqDdCS7gEOAn4bk5YDf6/JQjnnXE5MWFluUzHKpRfH3mY2lO3VAAAUwElEQVTWSdJ7AGa2SNL6NVwu55yrlA/YDz9KakA4FkjaDCir0VI551yO6vut3vcCzwGbS7oO+BVwXY2WyjnncmGirA5fJKw0QJvZYEkTgENj0glmNq1mi+Wcc5Uz/E5CgIbAj4TjUXe/rpxzRacut0Hn0ovjKuAJYGugNfC4pCtqumDOOZcLM+U0FaNcasMnA13N7GozuwrYEzilZovlnHM5sOrrBy1pA0njJE2W9H685oaktpLGSpoh6alULzZJjeP7mXF+m8S6rojpH0nqmUjvFdNmSrq8sjLlEqA/o3xTSCPg0xyWc865GmWIsrIGOU05WAkcbGbtgQ5AL0ndgVuB/mbWDlgMnBHznwEsNrMdgP4xH5J2AXoDuwK9gPskNZTUkNDp4jBgF+DXMW9G2QZL6k9oc14OvC9paHzfA3g7l711zrmaVl13CZqZAd/Ft+vFyYCDgd/E9EFAP+B+4Oj4GuBZ4B5JiulPmtlKYJakmYSWB4CZZvYpgKQnY94PMpUp20XCVE+N94FXEuljsu2kc87Vpiq0L7eUND7xfoCZDUhmiLXcCcAOhNruJ8A3ZlYSs8wFWsXXrYA5oQxWImkJsFlMT8bJ5DJz0tK7ZStwtsGSHsq2oHPO5ZtZlQL0QjPrkn19Vgp0kNQCeAHYuaJs8f+KNpw+AmgyvaJ2FqsgbZVKu9lJ2h64kdBmssGqtZrtWNmyzjlXs2pmICQz+0bSSKA70EJSo1iLbg3Mi9nmAtsAcyU1ApoDixLpKcllMqVXKJeW84HAI4RvhcOAp4Enc1jOOedqXHVdJJS0eaw5I6kJ4ea86cB/gONjtj7Ai/H1kPieOP+N2I49BOgde3m0BdoB44B3gXaxV8j6hAuJQ7KVKZcbVTY0s6GSbjezT4CrJb2Vw3LOOVejjOq7SAhsBQyK7dANgKfN7GVJHwBPSvoz8B6Qav59CPhnvAi4iBBwMbP3JT1NuPhXAvw+Np0g6VxgKOHmv4fN7P1sBcolQK+MVyY/kXQ28DmwRVX22jnnakTV2qCzr8psCtCxgvRPWd0LI5n+PXBChnXdSGgaTk9/FXg11zLlEqAvBJoC58UNNgdOz3UDzjlXc4p3MP5c5DJY0tj4cimrB+13zrm8M8LNKnVVthtVXiBLFxAzO65GSuScc1VQrONs5CJbDfqeWitFEZgwYRaNGvSpPKNzbi2UruVyorQ+DthvZiNqsyDOOVdVZtXai6Pg5DoetHPOFaT62sThnHMFr6w+XiRMJ6lxHJ3JOecKQl1/qncuT1TZU9JUYEZ8317S3TVeMuecq1S4SJjLVIxyKfVdwJHA1wBmNhk4qCYL5ZxzuTCgLMepGOXSxNHAzD4Ld3uvsrZ9YpxzrvpU463ehSiXAD1H0p6AxUFE/gB8XLPFcs653NT3bnbnEJo5tgUWAMNjmnPO5V29vNU7xcy+JA6j55xzhcQQJWX1OEBLeoAKxuQws741UiLnnKuCel2DJjRppGwAHEv5Bx8651xeVPOA/QUnlyaOp5LvJf0TGFZjJXLOuVxZGI+jrlqbW73bAttVd0Gcc25t1OtbvSUtZnUbdAPCs7cur8lCOedcLur6rd5ZA3R8FmF7wnMIAcriU2udc64AiNI6HKCz3uodg/ELZlYaJw/OzrmCkbpImMtUjHIZi2OcpE41XhLnnFsL9XIsDkmNzKwE2Bf4naRPgGWACJVrD9rOubyrr23Q44BOwDG1VBbnnKuS+vzIKwGY2Se1VBbnnKuyunyRMFuA3lzSRZlmmtlfa6A8zjmXs9R40HVVtgDdEGgKdbgXuHOuyKnetkHPN7Pra60kzjm3FuprDbrufi055+qE+nwn4SG1VgrnnFtLZXX49rmMN6qY2aLaLIhzzlWVWejFkctUGUnbSPqPpOmS3pd0fkzfVNIwSTPi/5vEdEm6S9JMSVOSN/RJ6hPzz5DUJ5HeWdLUuMxdSnvYa7rifBa5c85F1XgnYQlwsZntDHQHfi9pF8LgcCPMrB0wgtWDxR0GtItTX+B+CAEduBboBuwJXJsK6jFP38RyvbIVyAO0c66omSmnqfL12HwzmxhfLwWmA62Ao4FBMdsgVt+8dzQw2IIxQAtJWwE9gWFmtsjMFhPGz+8V5zUzs9FxXKPBVHIj4NqMB+2ccwWhiv2gW0oan3g/wMwGVJRRUhugIzAW2NLM5kMI4pK2iNlaUf7pUnNjWrb0uRWkZ+QB2jlX1KowxuZCM+tSWSZJTYHngAvM7NsszcQVzbC1SM/Imzicc0XLgBJTTlMuJK1HCM6PmdnzMXlBbJ4g/v9lTJ8LbJNYvDUwr5L01hWkZ+QB2jlX1MxymyoTe1Q8BExPG8piCJDqidEHeDGRfkrszdEdWBKbQoYCPSRtEi8O9gCGxnlLJXWP2zolsa4KeROHc66IqTqfSbgP8FtgqqRJMe1K4BbgaUlnAP8DTojzXgUOB2YCy4HTIHRRlnQD8G7Md32i2/I5wECgCfBanDLyAO2cK17V+FRvM3ubzHdQr3HjXuyJ8fsM63oYeLiC9PHAbrmWyQO0c65o1efR7JxzruDV5Vu9PUA754qWUX8H7HfOuYJXXW3QhcgDtHOuqHkbtHPOFaAwHnS+S1FzPEA754qa16Cdc64AhfGg812KmuMB2jlX1LybnXPOFShvg3bOuQLkdxI651wBq8tNHD7caD3x0EMPsGDBPKZOnbQq7bbbbmX69GlMnjyR559/lubNm+exhC7dBRecz7Rpk5k6dRKPP/4ojRs35uCDD2bChHG899543nprFNtvv32+i5lX4U7C3KZi5AG6nhg4cDC9eh1RLm3YsOHstlt72rfvxMcfz+CKKy7PsLSrbVtvvTXnnXcuXbp0Y/fdO9CwYUN69z6R+++/h5NOOoWOHbvw+ONPcPXVV+a7qPmV41jQxdpO7QG6nnjrrbdYtGhRubRhw4ZRWloKwJgxY2jdOuvj0Vwta9SoEU2aNKFhw4ZsuOGGzJs3HzOjWbNmADRv3px58+bnuZT5V41P9S443gbtADj99NN46qmn810MF82bN4/bb/8r//vfLFasWMHrrw9j2LBhnHnmWbz66kusWLGCb7/9lu7d98l3UfPK8DbovJFUKmlSYmpTA9toI2lada+3mFx55RWUlJTw2GOP57soLmrRogVHH/0L2rbdga233oaNNtqIk076DRdeeD6HH34U22zThkceGcRf/3p7vouad5bjVIwKvQa9wsw6ZJopqZGZldRmgeqaU075LUceeQSHHPLzfBfFJRx66CHMmjWLhQsXAvD88y+wzz570779HowbNw6Ap556mn//+5V8FrMgeA26gEg6VdIzkl4CXpfUVNIISRMlTZV0dMxXrmYs6RJJ/eLrzpImSxpNhkfW1Ac9e/bksssu5Re/OIYVK1bkuzgu4X//m0P37t1o0qQJAIcccjAffDCd5s2b065dOwB+/vNDmT79w3wWM+/qei+OQq9BN0k8vHGWmR0bX+8F7BEfztgIONbMvpXUEhgjaUgl630E+IOZjZL0l0yZJPUF+q7rThSCxx9/lAMPPICWLVsyZ85srr32Oq644jIaN27MsGH/BmDMmLGcc069/b4qKOPGjePZZ59n4sR3KSkp4b33JjFgwAPMnTuX5557mrKyMhYv/obTTz8z30XNu7Ji7aKRA1kB75yk78ysaVraqcABZnZafL8e0B/Yn3Cx9mdAW2AD4GUz2y3muwRoGvNONbNtY/oewOOpfFnKYtCwGvfOObdaKWZVfzTKpo22tEOanphT3meX3D3BzLpUuWh5VOg16EyWJV6fBGwOdDazHyXNJgTnEso34WwQ/xfFe83AOZdg5m3Qha458GUMzgcB28X0BcAWkjaT1Bg4EsDMvgGWSNo35jup1kvsnKsmlvO/YlSsNeikx4CXJI0HJgEfAsSAfT0wFpiVSo9OAx6WtBwYWsvldc5VEwNKijP25qSg26ALibdBO1eT1q4NukXDLWy/DX+VU96Xv7vX26Cdc662+HCjzjlXwOpyK4AHaOdcUfMatHPOFaAwWJLXoJ1zriCVFmkXulzUhX7Qzrl6yjDKcpwqI+lhSV+mjeGzqaRhkmbE/zeJ6ZJ0l6SZkqZI6pRYpk/MP0NSn0R65zhe0My4bKW9VjxAO+eKl4UmjlymHAwEeqWlXQ6MMLN2wIj4HuAwoF2c+gL3QwjowLVAN2BP4NpUUI95+iaWS9/WGjxAO+eKWnXdSWhmbwKL0pKPBgbF14OAYxLpgy0YA7SQtBXQExhmZovMbDEwDOgV5zUzs9EWup0MTqwrI2+Dds4VrdAPukbboLc0s/kAZjZf0hYxvRUwJ5FvbkzLlj63gvSsPEA754qYUZp7R7uWcUiIlAFmNmAtN1xR+7GtRXpWHqCdc0WrijXohWtxq/cCSVvF2vNWwJcxfS6wTSJfa2BeTD8wLX1kTG9dQf6svA3aOVe8BGUqy2laS0OAVE+MPsCLifRTYm+O7sCS2BQyFOghaZN4cbAHMDTOWyqpe+y9cUpiXRl5Ddo5V9Sqqw1a0hOE2m9LSXMJvTFuAZ6WdAbwP+CEmP1V4HBgJrCcMEIm8SlPNwDvxnzXm1nqwuM5hJ4iTYDX4pS9THX5Pvbq5KPZOVeT1m40uw0bbmY7bXB4TnnfW/6oj2bnnHO1qYZ7ceSVB2jnXNEyjBKV5LsYNcYDtHOuqJXV4fHsPEA754pYGI2jrvIA7ZwrWgbr0oWu4HmAds4VMfMmDuecK0SGUcqP+S5GjfEA7Zwral6Dds65ghTq0HWVB2jnXNEKgyV5Ddo55wqSd7NzzrkC5BcJnXOuYBll5m3QzjlXkLyJwznnCpL34nDOuYJkQJl5Ddo55wqQD5bknHMFycwoNe/F4ZxzBcm8icM55wqRXyR0zrmC5TVo55wrSH6R0DnnCpJhlJX5RULnnCtIXoN2zrlCZN4G7ZxzBcrboJ1zriAZYD6anXPOFaLwTJW6ygO0c66IGWVWku9C1BgP0M65Iuc1aOecK0zei8M55wqR9+JwzrkC5gHaOecKkPmNKg6AhVD6Wb4LUQNaAgvzXQiXs7p6vrZby+WGQknLHPMW3XGTmeW7DC6PJI03sy75LofLjZ+v+qVBvgvgnHOuYh6gnXOuQHmAdgPyXQBXJX6+6hFvg3bOuQLlNWjnnCtQHqCdc65AeYAuEpJM0h2J95dI6leF5U+V9JWkSXEaXEPl7CfpkppYd10iqTRxLiZJalMD22gjaVp1r9fVHr9RpXisBI6TdLOZrW2H+6fM7NxMMyU1MqvDYzcWlhVm1iHTTD8XDrwGXUxKCFfwL0yfIWk7SSMkTYn/b5vrSiWNlHSTpFHA+ZKOkjRW0nuShkvaMuYrVzOWNC1V65N0laSPJA0HfrZuu1l/xV85z0h6CXhdUtN4PidKmirp6JivXM04+WtKUmdJkyWNBn6flx1x1cYDdHG5FzhJUvO09HuAwWa2B/AYcFeG5U9M/KQ+LZHewswOMLM7gLeB7mbWEXgS+GO2AknqDPQGOgLHAV2rvFf1U5PEuXghkb4X0MfMDga+B441s07AQcAdklTJeh8BzjOzvWqm2K42eRNHETGzb2Pb8XnAisSsvQjBEeCfwG0ZVpGpieOpxOvWwFOStgLWB2ZVUqz9gBfMbDmApCGV5HdBpiaOYWa2KL4WcJOk/QlDtrUCtsy0wvjF3cLMRsWkfwKHVWOZXS3zGnTxuRM4A9goS56qdm5flnh9N3CPme0OnAVsENNLKP952SDx2jvTV5/kuTgJ2BzoHIP5AsJxz3QuhJ+LOsUDdJGJtaunCUE65R1CMwOEP+q312ETzYHP4+s+ifTZQCcASZ2AtjH9TeBYSU0kbQwctQ7bduU1B740sx8lHcTqEd8WAFtI2kxSY+BIADP7Blgiad+Y76RaL7GrVh6gi9MdhGEnU84DTpM0BfgtcP46rLsf8Iyktyg/PONzwKaSJgHnAB8DmNlEQhPJpJjnrXXYtivvMaCLpPGEYPshgJn9CFwPjAVeTqVHpwH3xouEK3BFzW/1ds65AuU1aOecK1AeoJ1zrkB5gHbOuQLlAdo55wqUB2jnnCtQHqDdOkuMzDYtjiWx4Tqs60BJL8fXv5B0eZa8LST931pso8IR93IZiU/SQEnHV2FbPqKcW2seoF11WGFmHcxsN+AH4OzkTAVV/qyZ2RAzuyVLlhZAlQO0c8XCA7Srbm8BO8Sa43RJ9wETgW0k9ZA0Oo7O9oykpgCSekn6UNLbrB5TJDW62z3x9ZaSXogjtU2WtDdwC7B9rL3/Jea7VNK7cWS/6xLrqtKIe5J+F9czWdJzab8KDpX0lqSPJR0Z8zeU9JfEts9a1wPpnAdoV20kNSIMzjM1Jv2MMMpeR8IYE1cDh8bR2cYDF0naAHiAcIv4fsBPMqz+LmCUmbUn3HL+PnA58EmsvV8qqQfQDtgT6AB0lrT/Wo6497yZdY3bm075W+vbAAcARwB/j/twBrDEzLrG9f9OUlucWwc+mp2rDk3iLeAQatAPAVsDn5nZmJjeHdgF+G8cMXN9YDSwEzDLzGYASHoU6FvBNg4GTgEws1LCmBObpOXpEaf34vumhIC9MVUfcW83SX8mNKM0BYYm5j1tZmXADEmfxn3oAeyRaJ9uHrf9cQ7bcq5CHqBddVhj6MwYhJMjs4kwlOav0/J1oPpGYBNws5n9I20bF6zFNgYCx5jZZEmnAgcm5qWvy+K2/2BmyUCOauBRVq7+8CYOV1vGAPtI2gFA0oaSdiQM9NNW0vYx368zLD+CMEhTqr23GbCUUDtOGQqcnmjbbiVpC9ZuxL2NgfmS1mPNUeFOkNQglvmnwEdx2+fE/EjaUVK2IWGdq5TXoF2tMLOvYk30iThEJsDVZvaxpL7AK5IWEoZK3a2CVZwPDJB0BlAKnGNmoyX9N3Zjey22Q+8MjI41+O+Ak81soqTUiHufkduIe38ijBb3GaFNPflF8BEwijB4/tlm9r2kBwlt0xMVNv4VcExuR8e5ivlods45V6C8icM55wqUB2jnnCtQHqCdc65AeYB2zrkC5QHaOecKlAdo55wrUB6gnXOuQP0/m2eRoWq8fQwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 360x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#Menerapkan confusion matrix pada testing set\n",
    "y_pred_log_reg = log_reg.predict(X_test2)\n",
    "\n",
    "\n",
    "#log\n",
    "#mengganti testing setnya dengan testing set untuk data keseluruhan\n",
    "log_reg_cf = confusion_matrix(y_test2, y_pred_log_reg)\n",
    "\n",
    "import itertools\n",
    "\n",
    "# Create a confusion matrix\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=0)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"black\" if cm[i, j] > thresh else \"white\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "labels = ['No Fraud', 'Fraud']\n",
    "\n",
    "fig = plt.figure(figsize=(5,4))\n",
    "\n",
    "plot_confusion_matrix(log_reg_cf, labels, title=\"Confusion Matrix Logistic Regression \\n Under Sampling {}\" .format(nm.__class__.__name__) , cmap=plt.cm.inferno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G-Mean untuk ENN:  0.9225038376719651\n"
     ]
    }
   ],
   "source": [
    "G_mean_under_sampling_enn = geometric_mean_score (y_test2, y_pred_log_reg)\n",
    "\n",
    "print (\"G-Mean untuk ENN: \", G_mean_under_sampling_enn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression dengan Down Sampling:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98     56862\n",
      "           1       0.04      0.88      0.09       100\n",
      "\n",
      "   micro avg       0.97      0.97      0.97     56962\n",
      "   macro avg       0.52      0.92      0.53     56962\n",
      "weighted avg       1.00      0.97      0.98     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Logistic Regression dengan Down Sampling:')\n",
    "print(classification_report(y_test2, y_pred_log_reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
